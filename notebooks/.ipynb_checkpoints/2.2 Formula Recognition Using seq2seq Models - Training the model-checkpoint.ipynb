{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Formula Recognition Using seq2seq Models - Training the model\n",
    "\n",
    "In this notebook, we train a seq2seq model that transcribes typeset formulas from images into Latex markup. The model consists of a few components:\n",
    "\n",
    "#### An encoder \n",
    "  - This is the same model as we used in the previous notebook, consisting of 3 convolutional layers with increasing hidden units.\n",
    "  \n",
    "#### A decoder \n",
    "  - the decoder takes the features from our encoder, and generates an \"embedding\" for them, which is a fixed-sized vector that represents the visual content of the input image. We apply Bahdanau attention over this feature vector, and concatenate it with the hidden state of a GRU. This is then fed back into a GRU. Finally, we use two fully-connected layers where the last layer has a softmax activation in order to produce our final prediction.\n",
    "  \n",
    "#### References\n",
    "- [Image Captioning With Attention](https://www.tensorflow.org/beta/tutorials/text/image_captioning)\n",
    "- [im2latex paper](http://arxiv.org/pdf/1609.04938v1.pdf)\n",
    "- [Guillaume Genthial's im2latex implementation](https://github.com/guillaumegenthial/im2latex)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta1\n",
      "False\n",
      "False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(tf.test.is_gpu_available())\n",
    "print(tf.test.is_built_with_cuda())\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, the directory structure is defined as:\n",
    "```\n",
    "project/\n",
    "    data/ -- Contains the data used in the project (both original and derived)\n",
    "    doc/ -- Project documentation (including report & summary)\n",
    "    figs/ -- Any saved figures generated by the project\n",
    "    notebooks/ -- All notebooks for the project\n",
    "    scripts/ -- Scripts used for various reasons, such as pre-processing\n",
    "    src/ -- Regular python code\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "### Make sure our data is in order\n",
    "data_base_dir = \"../data\"\n",
    "figs_base_dir = \"../figs\"\n",
    "\n",
    "original_data_path = data_base_dir + \"/original/formula/\"\n",
    "processed_data_path = data_base_dir + \"/processed/formula/\"\n",
    "pickle_data_path = data_base_dir + \"/pickle/formula/\"\n",
    "\n",
    "assert os.path.exists(original_data_path), \"Original data path does not exist.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "def natural_keys(text):\n",
    "    '''\n",
    "    alist.sort(key=natural_keys) sorts in human order\n",
    "    http://nedbatchelder.com/blog/200712/human_sorting.html\n",
    "    (See Toothy's implementation in the comments)\n",
    "    '''\n",
    "    return [ atoi(c) for c in re.split(r'(\\d+)', text) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17 training images.\n",
      "Found 17 validation images.\n",
      "Found 17 test images.\n"
     ]
    }
   ],
   "source": [
    "training_images = glob.glob(f\"{processed_data_path}/images/train/*.png\")\n",
    "training_images.sort(key=natural_keys)\n",
    "validation_images = glob.glob(f\"{processed_data_path}/images/validate/*.png\")\n",
    "validation_images.sort(key=natural_keys)\n",
    "test_images = glob.glob(f\"{processed_data_path}/images/test/*.png\")\n",
    "test_images.sort(key=natural_keys)\n",
    "\n",
    "print(f\"Found {len(training_images)} training images.\")\n",
    "print(f\"Found {len(validation_images)} validation images.\")\n",
    "print(f\"Found {len(test_images)} test images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 17 training labels.\n",
      "Got 17 validation labels.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "def load_labels(labels_path, matches_path):\n",
    "    with open(labels_path) as f:\n",
    "        labels = np.array(f.read().splitlines())\n",
    "    matches =  pd.read_csv(matches_path, sep=' ', header=None).values\n",
    "    return labels[[list(map(lambda f: f[1], matches))][0]]\n",
    "\n",
    "train_labels_path = f\"{original_data_path}train.formulas.norm.txt\"    \n",
    "train_matches_path = f\"{processed_data_path}images/train/train.matching.txt\"\n",
    "train_labels = load_labels(train_labels_path, train_matches_path)\n",
    "print(f\"Got {len(train_labels)} training labels.\")\n",
    "\n",
    "validate_labels_path = f\"{original_data_path}val.formulas.norm.txt\"    \n",
    "validate_matches_path = f\"{processed_data_path}images/validate/val.matching.txt\"\n",
    "validate_labels = load_labels(validate_labels_path, validate_matches_path)\n",
    "print(f\"Got {len(validate_labels)} validation labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vocabulary class encapsulates the vocabulary (in other words, all possible tokens). It also has methods for tokenizing, padding, and performing a reverse lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "    def __init__(self, vocab_path):\n",
    "        self.build_vocab(vocab_path)\n",
    "        \n",
    "    def build_vocab(self, vocab_path):\n",
    "        '''\n",
    "        Builds the complete vocabulary, including special tokens\n",
    "        '''\n",
    "        self.unk   = \"<UNK>\"\n",
    "        self.start = \"<SOS>\"\n",
    "        self.end   = \"<END>\"\n",
    "        self.pad   = \"<PAD>\"\n",
    "        \n",
    "        # First, load our vocab from disk & determine \n",
    "        # highest index in mapping.\n",
    "        vocab = self.load_vocab(vocab_path)\n",
    "        max_index = max(vocab.values())\n",
    "        \n",
    "        # Compile special token mapping\n",
    "        special_tokens = {\n",
    "            self.unk : max_index + 1,\n",
    "            self.start : max_index + 2,\n",
    "            self.end : max_index + 3,\n",
    "            self.pad : max_index + 4\n",
    "        }\n",
    "        \n",
    "        # Merge dicts to produce final word index\n",
    "        self.token_index = {**vocab, **special_tokens}\n",
    "        self.reverse_index = {v: k for k, v in self.token_index.items()} \n",
    "\n",
    "    def load_vocab(self, vocab_path):\n",
    "        '''\n",
    "        Load vocabulary from file\n",
    "        '''\n",
    "        token_index = {}\n",
    "        with open(vocab_path) as f:\n",
    "            for idx, token in enumerate(f):\n",
    "                token = token.strip()\n",
    "                token_index[token] = idx\n",
    "        assert len(token_index) > 0, \"Could not build word index\"\n",
    "        return token_index\n",
    "                \n",
    "    def tokenize_formula(self, formula):\n",
    "        '''\n",
    "        Converts a formula into a sequence of tokens using the vocabulary\n",
    "        '''\n",
    "        def lookup_token(token):\n",
    "            return self.token_index[token] if token in self.token_index else self.token_index[self.unk]\n",
    "        tokens = formula.strip().split(' ')        \n",
    "        return list(map(lambda f: lookup_token(f), tokens))\n",
    "        \n",
    "    def pad_formula(self, formula, max_length):\n",
    "        '''\n",
    "        Pads a formula to max_length with pad_token, appending end_token.\n",
    "        '''\n",
    "        # Extra space for the end token\n",
    "        padded_formula = self.token_index[self.pad] * np.ones(max_length + 1)\n",
    "        padded_formula[len(formula)] = self.token_index[self.end]\n",
    "        padded_formula[:len(formula)] = formula\n",
    "        return padded_formula\n",
    "\n",
    "    @property\n",
    "    def length(self):\n",
    "        return len(self.token_index)\n",
    "    \n",
    "vocab = Vocab(f\"{processed_data_path}/vocab.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "While a full hyperparameter search was not performed, the below hyperparameters seemed to get reasonable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 1000\n",
    "batch_size = 16\n",
    "embedding_dim = 256\n",
    "vocab_size = vocab.length\n",
    "hidden_units = 256\n",
    "num_datapoints = 35000\n",
    "num_training_steps = num_datapoints // batch_size\n",
    "num_validation_datapoints = 8474\n",
    "num_validation_steps = num_validation_datapoints // batch_size\n",
    "epochs = 50\n",
    "train_new_model = False\n",
    "max_image_size=(80,400)\n",
    "max_formula_length = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "\n",
    "Tensorflow provides the `tf.data.Dataset` API for doing ETL -- loading, transforming and streaming your data to the appropriate compute device. Originally, our implementation used regular numpy based processing, but that had some limitations:\n",
    "- No parallel processing -- bound by the performance of a single thread\n",
    "- No streaming operations -- all data must be put into memory\n",
    "\n",
    "Using the `tf.data.Dataset` API had it's own limitations; mainly that it was somewhat tricky to rewrite the operations to be nodes in the Tensorflow graph.\n",
    "\n",
    "Note that this API also allows you to distribute your data easily to multiple GPUs or TPUs - a fact that will come in handy once we start training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This hash table is used to perform token lookups in the vocab\n",
    "table = tf.lookup.StaticHashTable(\n",
    "    initializer=tf.lookup.KeyValueTensorInitializer(\n",
    "        keys=tf.constant(list(vocab.token_index.keys())),\n",
    "        values=tf.constant(list(vocab.token_index.values())),\n",
    "    ),\n",
    "    default_value=tf.constant(vocab.token_index[vocab.unk]),\n",
    "    name=\"class_weight\"\n",
    ")\n",
    "\n",
    "def load_and_decode_img(path):\n",
    "    ''' Load the image and decode from png'''\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_png(image)\n",
    "    return tf.image.rgb_to_grayscale(image)\n",
    "\n",
    "@tf.function\n",
    "def lookup_token(token):\n",
    "    ''' Lookup the given token in the vocab'''\n",
    "    table.lookup(token)\n",
    "    return  table.lookup(token)\n",
    "\n",
    "def process_label(label):\n",
    "    ''' Split to tokens, lookup & append <END> token'''\n",
    "    tokens = tf.strings.split(label, \" \")   \n",
    "    tokens = tf.map_fn(lookup_token, tokens, dtype=tf.int32)\n",
    "    return tf.concat([tokens, [vocab.token_index[vocab.end]]], 0)\n",
    "\n",
    "def process_datum(path, label):\n",
    "    return load_and_decode_img(path), process_label(label)\n",
    "\n",
    "# Tokenize formulas\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((training_images, train_labels)).map(process_datum, num_parallel_calls=8)\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((validation_images, validate_labels)).map(process_datum, num_parallel_calls=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Visualize\" what data we have before filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[3 3 1 3 2 3 3 3 3 3 3 1 3 3 3 2 3 1 3 2 3 3 3 1 3 2 3 3 3 3 1 3 1 3 2 1 3\n",
      " 3 1 3 3 3 2 2 2 3 3 3 5], shape=(49,), dtype=int32)\n",
      "\n",
      "\n",
      "tf.Tensor([3 3 3 3 3 3 1 3 2 1 3 3 3 1 3 2 2 3 3 1 3 3 2 3 1 3 2 1 3 3 3 1 3 2 2 3 5], shape=(37,), dtype=int32)\n",
      "\n",
      "\n",
      "tf.Tensor([3 3 1 3 2 3 3 3 3 3 3 3 3 3 3 3 3 5], shape=(18,), dtype=int32)\n",
      "\n",
      "\n",
      "tf.Tensor([3 3 3 1 3 2 3 3 1 3 2 3 3 3 1 0 2 3 1 3 3 2 3 3 5], shape=(25,), dtype=int32)\n",
      "\n",
      "\n",
      "tf.Tensor(\n",
      "[1 3 1 3 3 2 1 3 3 2 2 3 3 1 3 1 3 3 2 1 3 3 1 3 3 1 3 2 2 3 1 0 2 2 2 3 3\n",
      " 5], shape=(38,), dtype=int32)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print some values from the dataset (pre-filter)\n",
    "for datum in train_dataset.take(5):\n",
    "    print(datum[1])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to exclude large images because they will make the training process slower, as well as long formulas. \n",
    "Due to the [vanishing gradient problem](https://cs224d.stanford.edu/lectures/CS224d-Lecture8.pdf), long formulas will likely not be predicted very well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_size(image, label):\n",
    "    '''Filter the dataset by the size of the image & length of label'''\n",
    "    label_length = tf.shape(label)\n",
    "    image_size = tf.shape(image)\n",
    "    \n",
    "    # Does this image meet our size constraint?\n",
    "    keep_image = tf.math.reduce_all(\n",
    "        tf.math.greater_equal(max_image_size, image_size[:2])\n",
    "    )\n",
    "    # Does this image meet our formula length constraint?\n",
    "    keep_label = tf.math.reduce_all(\n",
    "        tf.math.greater_equal(max_formula_length, label_length[0])\n",
    "    )\n",
    "    return tf.math.logical_and(keep_image, keep_label)\n",
    "\n",
    "train_dataset = train_dataset.filter(filter_by_size).take(num_datapoints)\n",
    "validation_dataset = validation_dataset.filter(filter_by_size).take(num_validation_datapoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[3 3 1 3 2 3 3 3 3 3 3 1 3 3 3 2 3 1 3 2 3 3 3 1 3 2 3 3 3 3 1 3 1 3 2 1 3\n",
      " 3 1 3 3 3 2 2 2 3 3 3 5], shape=(49,), dtype=int32)\n",
      "\n",
      "\n",
      "tf.Tensor([3 3 3 3 3 3 1 3 2 1 3 3 3 1 3 2 2 3 3 1 3 3 2 3 1 3 2 1 3 3 3 1 3 2 2 3 5], shape=(37,), dtype=int32)\n",
      "\n",
      "\n",
      "tf.Tensor([3 3 1 3 2 3 3 3 3 3 3 3 3 3 3 3 3 5], shape=(18,), dtype=int32)\n",
      "\n",
      "\n",
      "tf.Tensor([3 3 3 1 3 2 3 3 1 3 2 3 3 3 1 0 2 3 1 3 3 2 3 3 5], shape=(25,), dtype=int32)\n",
      "\n",
      "\n",
      "tf.Tensor(\n",
      "[1 3 1 3 3 2 1 3 3 2 2 3 3 1 3 1 3 3 2 1 3 3 1 3 3 1 3 2 2 3 1 0 2 2 2 3 3\n",
      " 5], shape=(38,), dtype=int32)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for datum in train_dataset.take(5):\n",
    "    print(datum[1])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leverage the power of the `tf.data.Dataset` API to shuffle, batch, pad and cache, all in just a few lines of python code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and batch (dropping any batches that are < BATCH_SIZE long)\n",
    "# also pad each batch to the largest image size + formulas to\n",
    "# max_formula_length.\n",
    "shapes = (tf.TensorShape([None,None,1]),tf.TensorShape([max_formula_length]))\n",
    "values = (tf.constant(255, dtype=tf.uint8), tf.constant(vocab.token_index[vocab.pad]))\n",
    "train_dataset = train_dataset.shuffle(buffer_size).padded_batch(\n",
    "    batch_size, \n",
    "    padded_shapes=shapes,\n",
    "    padding_values=values,\n",
    "    drop_remainder=True\n",
    ")\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE).cache()\n",
    "\n",
    "validation_dataset = validation_dataset.shuffle(buffer_size).padded_batch(\n",
    "    batch_size, \n",
    "    padded_shapes=shapes,\n",
    "    padding_values=values,\n",
    "    drop_remainder=True\n",
    ")\n",
    "\n",
    "# Prefetch and cache for performance\n",
    "validation_dataset = validation_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random \n",
    "import textwrap\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def plot_example(example):\n",
    "    ''' Plot a single example'''\n",
    "    if example is None: \n",
    "        return\n",
    "    \n",
    "    image_tensor, label = example\n",
    "    \n",
    "    label = \"\".join([vocab.reverse_index[token.numpy()] for token in label])\n",
    "    label = label.replace(\"<PAD>\",\"\")\n",
    "    \n",
    "    fig = plt.figure(figsize=(20,20))\n",
    "    plt.imshow(image_tensor[:,:,0], cmap='gray')\n",
    "    plt.title(textwrap.wrap(label,100))\n",
    "    plt.show()\n",
    "\n",
    "# Plot a few image from a random batch\n",
    "for img_tensor, labels in train_dataset.take(5):\n",
    "    if len(img_tensor.shape) == 4:\n",
    "        image = img_tensor[0, :, :, :]\n",
    "        label = labels[0]\n",
    "        plot_example((image, label)) \n",
    "    else:\n",
    "        plot_example((img_tensor, labels)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition\n",
    "\n",
    "Below are the components of the network:\n",
    "- The timing signal function -- adds sinusoids to the features so that our flattening operations doesn't mean we lose a sense of order\n",
    "- Bahdanau attenntion\n",
    "- The encoder (from the last notebook)\n",
    "- The decoder (using the timing signal function and the attention layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import math\n",
    "import numpy as np\n",
    "from six.moves import xrange\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# taken from https://github.com/tensorflow/tensor2tensor/blob/37465a1759e278e8f073cd04cd9b4fe377d3c740/tensor2tensor/layers/common_attention.py\n",
    "def add_timing_signal_nd(x, min_timescale=1.0, max_timescale=1.0e4):\n",
    "    \"\"\"Adds a bunch of sinusoids of different frequencies to a Tensor.\n",
    "\n",
    "    Each channel of the input Tensor is incremented by a sinusoid of a difft\n",
    "    frequency and phase in one of the positional dimensions.\n",
    "\n",
    "    This allows attention to learn to use absolute and relative positions.\n",
    "    Timing signals should be added to some precursors of both the query and the\n",
    "    memory inputs to attention.\n",
    "\n",
    "    The use of relative position is possible because sin(a+b) and cos(a+b) can\n",
    "    be expressed in terms of b, sin(a) and cos(a).\n",
    "\n",
    "    x is a Tensor with n \"positional\" dimensions, e.g. one dimension for a\n",
    "    sequence or two dimensions for an image\n",
    "\n",
    "    We use a geometric sequence of timescales starting with\n",
    "    min_timescale and ending with max_timescale.  The number of different\n",
    "    timescales is equal to channels // (n * 2). For each timescale, we\n",
    "    generate the two sinusoidal signals sin(timestep/timescale) and\n",
    "    cos(timestep/timescale).  All of these sinusoids are concatenated in\n",
    "    the channels dimension.\n",
    "\n",
    "    Args:\n",
    "        x: a Tensor with shape [batch, d1 ... dn, channels]\n",
    "        min_timescale: a float\n",
    "        max_timescale: a float\n",
    "\n",
    "    Returns:\n",
    "        a Tensor the same shape as x.\n",
    "\n",
    "    \"\"\"\n",
    "    static_shape = x.get_shape().as_list()\n",
    "    num_dims = len(static_shape) - 2\n",
    "    channels = tf.shape(x)[-1]\n",
    "    num_timescales = channels // (num_dims * 2)\n",
    "    log_timescale_increment = (\n",
    "            math.log(float(max_timescale) / float(min_timescale)) /\n",
    "            (tf.cast(num_timescales, tf.float32) - 1))\n",
    "    inv_timescales = min_timescale * tf.exp(\n",
    "            tf.cast(tf.range(num_timescales), tf.float32) * -log_timescale_increment)\n",
    "    for dim in xrange(num_dims):\n",
    "        length = tf.shape(x)[dim + 1]\n",
    "        position = tf.cast(tf.range(length), tf.float32)\n",
    "        scaled_time = tf.expand_dims(position, 1) * tf.expand_dims(\n",
    "                inv_timescales, 0)\n",
    "        signal = tf.concat([tf.sin(scaled_time), tf.cos(scaled_time)], axis=1)\n",
    "        prepad = dim * 2 * num_timescales\n",
    "        postpad = channels - (dim + 1) * 2 * num_timescales\n",
    "        signal = tf.pad(signal, [[0, 0], [prepad, postpad]])\n",
    "        for _ in xrange(1 + dim):\n",
    "            signal = tf.expand_dims(signal, 0)\n",
    "        for _ in xrange(num_dims - 1 - dim):\n",
    "            signal = tf.expand_dims(signal, -2)\n",
    "        x += signal\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import metrics, layers, Model\n",
    "\n",
    "class BahdanauAttention(layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, features, hidden):\n",
    "        # First, flatten the image\n",
    "        shape = tf.shape(features)\n",
    "        if len(shape) == 4:\n",
    "            batch_size = shape[0]\n",
    "            img_height = shape[1]\n",
    "            img_width  = shape[2]\n",
    "            channels   = shape[3]\n",
    "            features = tf.reshape(features, shape=(batch_size, img_height*img_width, channels))\n",
    "        else:\n",
    "            print(f\"Image shape not supported: {shape}.\")\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        # features(CNN_encoder_output)\n",
    "        # shape => (batch_size, flattened_image_size, embedding_size)\n",
    "        \n",
    "        # hidden shape == (batch_size, hidden_size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        # score \n",
    "        # shape => (batch_size, flattened_image_size, hidden_size)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "        # attention_weights\n",
    "        # shape => (batch_size, flattened_image_size, 1)\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        \n",
    "        # context_vector\n",
    "        # shape after sum => (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        return context_vector, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNEncoder(tf.keras.Model):    \n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNNEncoder, self).__init__()\n",
    "        \n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "        \n",
    "    def build(self, input_shape):       \n",
    "        self.cnn_1 = layers.Conv2D(64, (3, 3), activation='relu', input_shape=(input_shape[1], input_shape[2], 1))\n",
    "        self.max_pool_1 = layers.MaxPooling2D((2, 2))\n",
    "        \n",
    "        self.cnn_2 = layers.Conv2D(256, (3, 3), activation='relu')\n",
    "        self.max_pool_2 = layers.MaxPooling2D((2, 2))\n",
    "        \n",
    "        self.cnn_3 = layers.Conv2D(512, (3, 3), activation='relu')\n",
    "        \n",
    "    def call(self, images):\n",
    "        images = tf.cast(images, tf.float32)\n",
    "        x = self.cnn_1(images)\n",
    "        x = self.max_pool_1(x)\n",
    "        x = self.cnn_2(x)\n",
    "        x = self.max_pool_2(x)\n",
    "        x = self.cnn_3(x)\n",
    "        return add_timing_signal_nd(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNDecoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNNDecoder, self).__init__()\n",
    "        self.units = units\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        \n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "        \n",
    "    def call(self, x, features, hidden):        \n",
    "        # attend over the image features\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "        \n",
    "        # convert our input vector to an embedding\n",
    "        x = self.embedding(x)\n",
    "                \n",
    "        # concat the embedding and the context vector (from attention)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # pass to GRU\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        x = self.fc1(output)\n",
    "        \n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "        \n",
    "        # This produces a distribution over the vocab\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x, state, attention_weights\n",
    "    \n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNNEncoder(embedding_dim=embedding_dim)\n",
    "decoder = RNNDecoder(embedding_dim, hidden_units, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function \n",
    "\n",
    "Define a loss function for our model: Sparse categorical cross-entropy is appropriate here since we're making multi-class predictions using integer labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    # In order to avoid <PAD> tokens contributing to the loss, we mask those tokens.\n",
    "    # First, we create the mask, and compute the loss.\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, vocab.token_index[vocab.pad]))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    # Second, we multiply the computed loss by the mask to zero out contributions from the <PAD> tokens.\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoints allow us to save state during training -- a real boon since training will take quite a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not restore from a checkpoint -- training new model!\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "checkpoint = tf.train.Checkpoint(encoder=encoder,\n",
    "                                 decoder=decoder,\n",
    "                                 optimizer=optimizer)\n",
    "checkpoint_manager = tf.train.CheckpointManager(checkpoint, checkpoint_path, max_to_keep=10)\n",
    "\n",
    "# Attempt to restore from training checkpoint\n",
    "start_epoch = 0\n",
    "save_at_epoch = 5\n",
    "if train_new_model is False and checkpoint_manager.latest_checkpoint:\n",
    "    checkpoint.restore(checkpoint_manager.latest_checkpoint)\n",
    "    start_epoch = int(checkpoint_manager.latest_checkpoint.split('-')[-1])*save_at_epoch\n",
    "    print(f\"Restored from checkpoint: {checkpoint_manager.latest_checkpoint}.\")\n",
    "    print(f\"Start epoch: {start_epoch}.\")\n",
    "else:\n",
    "    print(\"Did not restore from a checkpoint -- training new model!\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training \n",
    "Finally, we define a custom training and validation loop for our model. This is heavily inspired by the Tensorflow example linked at the top the notebook. Notable is the use of teacher forcing (described nit he code comment below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epoch_losses = []\n",
    "validation_epoch_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    ''' Function that encapsulates training logic'''\n",
    "    loss = 0\n",
    "\n",
    "    # reset the decoder state, since Latex is different for each image    \n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "    # shape => (batch_size, 1)\n",
    "    dec_input = tf.expand_dims([vocab.token_index[vocab.start]] * batch_size, 1)\n",
    "\n",
    "    sequence_length = target.shape[1]    \n",
    "    with tf.GradientTape() as tape:\n",
    "        features = encoder(img_tensor)\n",
    "\n",
    "        for i in range(0, sequence_length):\n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "            ground_truth_token = target[:, i]\n",
    "            loss += loss_function(ground_truth_token, predictions)\n",
    "                \n",
    "            # Teacher forcing: feed the correct word in as the next input to the\n",
    "            # encoder, to provide the decoder with the proper context to predict\n",
    "            # the following token in the sequence\n",
    "            dec_input = tf.expand_dims(ground_truth_token, 1)\n",
    "\n",
    "    total_loss = (loss / int(sequence_length))\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def validate_step(img_tensor, target):\n",
    "    ''' Function that encapsulates training logic'''\n",
    "    loss = 0\n",
    "\n",
    "    # reset the decoder state, since Latex is different for each image    \n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "    # shape => (batch_size, 1)\n",
    "    dec_input = tf.expand_dims([vocab.token_index[vocab.start]] * batch_size, 1)\n",
    "\n",
    "    sequence_length = target.shape[1]    \n",
    "    features = encoder(img_tensor)\n",
    "    for i in range(0, sequence_length):\n",
    "        # passing the features through the decoder\n",
    "        predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "        ground_truth_token = target[:, i]\n",
    "        loss += loss_function(ground_truth_token, predictions)\n",
    "        dec_input = tf.expand_dims(ground_truth_token, 1)\n",
    "\n",
    "    total_loss = (loss / int(sequence_length))\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Started training at: 2019-08-04 16:21:03.009617. Training for 50 epochs.]\n",
      "[Starting epoch: 0]\n",
      "[Epoch: 1 | Batch: 0 | Loss: 0.4435]\n",
      "[Epoch: 1 | Training loss: 0.44353583455085754]\n",
      "[Epoch: 1 | Validation loss: 0.37624841928482056]\n",
      "[Time elapsed for epoch: 7.354931831359863 seconds.] \n",
      "\n",
      "[Epoch: 2 | Batch: 0 | Loss: 0.4427]\n",
      "[Epoch: 2 | Training loss: 0.4427144527435303]\n",
      "[Epoch: 2 | Validation loss: 0.3755333721637726]\n",
      "[Time elapsed for epoch: 7.810528993606567 seconds.] \n",
      "\n",
      "[Epoch: 3 | Batch: 0 | Loss: 0.4422]\n",
      "[Epoch: 3 | Training loss: 0.44224634766578674]\n",
      "[Epoch: 3 | Validation loss: 0.374250203371048]\n",
      "[Time elapsed for epoch: 8.287670135498047 seconds.] \n",
      "\n",
      "[Epoch: 4 | Batch: 0 | Loss: 0.4426]\n",
      "[Epoch: 4 | Training loss: 0.4425501227378845]\n",
      "[Epoch: 4 | Validation loss: 0.3740517497062683]\n",
      "[Time elapsed for epoch: 8.043996095657349 seconds.] \n",
      "\n",
      "[Epoch: 5 | Batch: 0 | Loss: 0.4422]\n",
      "[Epoch: 5 | Training loss: 0.4422219395637512]\n",
      "[Epoch: 5 | Validation loss: 0.37476247549057007]\n",
      "[Time elapsed for epoch: 8.194286108016968 seconds.] \n",
      "\n",
      "[Epoch: 6 | Batch: 0 | Loss: 0.4415]\n",
      "[Epoch: 6 | Training loss: 0.44147956371307373]\n",
      "[Epoch: 6 | Validation loss: 0.374737024307251]\n",
      "[Time elapsed for epoch: 8.825609922409058 seconds.] \n",
      "\n",
      "[Epoch: 7 | Batch: 0 | Loss: 0.4413]\n",
      "[Epoch: 7 | Training loss: 0.4412896931171417]\n",
      "[Epoch: 7 | Validation loss: 0.3737833797931671]\n",
      "[Time elapsed for epoch: 7.8914289474487305 seconds.] \n",
      "\n",
      "[Epoch: 8 | Batch: 0 | Loss: 0.4413]\n",
      "[Epoch: 8 | Training loss: 0.44131287932395935]\n",
      "[Epoch: 8 | Validation loss: 0.3733982741832733]\n",
      "[Time elapsed for epoch: 8.1215980052948 seconds.] \n",
      "\n",
      "[Epoch: 9 | Batch: 0 | Loss: 0.4413]\n",
      "[Epoch: 9 | Training loss: 0.4412730634212494]\n",
      "[Epoch: 9 | Validation loss: 0.37381914258003235]\n",
      "[Time elapsed for epoch: 8.083650588989258 seconds.] \n",
      "\n",
      "[Epoch: 10 | Batch: 0 | Loss: 0.4407]\n",
      "[Epoch: 10 | Training loss: 0.44069015979766846]\n",
      "[Epoch: 10 | Validation loss: 0.3740895092487335]\n",
      "[Time elapsed for epoch: 7.770492076873779 seconds.] \n",
      "\n",
      "[Epoch: 11 | Batch: 0 | Loss: 0.4408]\n",
      "[Epoch: 11 | Training loss: 0.44075649976730347]\n",
      "[Epoch: 11 | Validation loss: 0.3731677830219269]\n",
      "[Time elapsed for epoch: 8.106982946395874 seconds.] \n",
      "\n",
      "[Epoch: 12 | Batch: 0 | Loss: 0.4406]\n",
      "[Epoch: 12 | Training loss: 0.4405675232410431]\n",
      "[Epoch: 12 | Validation loss: 0.3726901710033417]\n",
      "[Time elapsed for epoch: 8.382681846618652 seconds.] \n",
      "\n",
      "[Epoch: 13 | Batch: 0 | Loss: 0.4403]\n",
      "[Epoch: 13 | Training loss: 0.4403032064437866]\n",
      "[Epoch: 13 | Validation loss: 0.3734489679336548]\n",
      "[Time elapsed for epoch: 8.850203037261963 seconds.] \n",
      "\n",
      "[Epoch: 14 | Batch: 0 | Loss: 0.4400]\n",
      "[Epoch: 14 | Training loss: 0.44002479314804077]\n",
      "[Epoch: 14 | Validation loss: 0.37330353260040283]\n",
      "[Time elapsed for epoch: 9.026417970657349 seconds.] \n",
      "\n",
      "[Epoch: 15 | Batch: 0 | Loss: 0.4398]\n",
      "[Epoch: 15 | Training loss: 0.43975433707237244]\n",
      "[Epoch: 15 | Validation loss: 0.3729454278945923]\n",
      "[Time elapsed for epoch: 9.040682077407837 seconds.] \n",
      "\n",
      "[Epoch: 16 | Batch: 0 | Loss: 0.4405]\n",
      "[Epoch: 16 | Training loss: 0.44052115082740784]\n",
      "[Epoch: 16 | Validation loss: 0.37580397725105286]\n",
      "[Time elapsed for epoch: 9.117017030715942 seconds.] \n",
      "\n",
      "[Epoch: 17 | Batch: 0 | Loss: 0.4410]\n",
      "[Epoch: 17 | Training loss: 0.4410177767276764]\n",
      "[Epoch: 17 | Validation loss: 0.37464654445648193]\n",
      "[Time elapsed for epoch: 8.18964409828186 seconds.] \n",
      "\n",
      "[Epoch: 18 | Batch: 0 | Loss: 0.4401]\n",
      "[Epoch: 18 | Training loss: 0.4401036202907562]\n",
      "[Epoch: 18 | Validation loss: 0.3729134500026703]\n",
      "[Time elapsed for epoch: 8.120176792144775 seconds.] \n",
      "\n",
      "[Epoch: 19 | Batch: 0 | Loss: 0.4407]\n",
      "[Epoch: 19 | Training loss: 0.4407057762145996]\n",
      "[Epoch: 19 | Validation loss: 0.3725042939186096]\n",
      "[Time elapsed for epoch: 8.048460721969604 seconds.] \n",
      "\n",
      "[Epoch: 20 | Batch: 0 | Loss: 0.4384]\n",
      "[Epoch: 20 | Training loss: 0.4383624494075775]\n",
      "[Epoch: 20 | Validation loss: 0.37625810503959656]\n",
      "[Time elapsed for epoch: 7.9757399559021 seconds.] \n",
      "\n",
      "[Epoch: 21 | Batch: 0 | Loss: 0.4406]\n",
      "[Epoch: 21 | Training loss: 0.4406139850616455]\n",
      "[Epoch: 21 | Validation loss: 0.37341588735580444]\n",
      "[Time elapsed for epoch: 7.940975904464722 seconds.] \n",
      "\n",
      "[Epoch: 22 | Batch: 0 | Loss: 0.4387]\n",
      "[Epoch: 22 | Training loss: 0.43874701857566833]\n",
      "[Epoch: 22 | Validation loss: 0.3720952868461609]\n",
      "[Time elapsed for epoch: 7.555475950241089 seconds.] \n",
      "\n",
      "[Epoch: 23 | Batch: 0 | Loss: 0.4387]\n",
      "[Epoch: 23 | Training loss: 0.4387262165546417]\n",
      "[Epoch: 23 | Validation loss: 0.3719808757305145]\n",
      "[Time elapsed for epoch: 8.023620843887329 seconds.] \n",
      "\n",
      "[Epoch: 24 | Batch: 0 | Loss: 0.4389]\n",
      "[Epoch: 24 | Training loss: 0.4389096200466156]\n",
      "[Epoch: 24 | Validation loss: 0.3723534345626831]\n",
      "[Time elapsed for epoch: 8.237388134002686 seconds.] \n",
      "\n",
      "[Epoch: 25 | Batch: 0 | Loss: 0.4380]\n",
      "[Epoch: 25 | Training loss: 0.43801844120025635]\n",
      "[Epoch: 25 | Validation loss: 0.37360063195228577]\n",
      "[Time elapsed for epoch: 8.209336757659912 seconds.] \n",
      "\n",
      "[Epoch: 26 | Batch: 0 | Loss: 0.4383]\n",
      "[Epoch: 26 | Training loss: 0.4382511079311371]\n",
      "[Epoch: 26 | Validation loss: 0.37226414680480957]\n",
      "[Time elapsed for epoch: 8.090861082077026 seconds.] \n",
      "\n",
      "[Epoch: 27 | Batch: 0 | Loss: 0.4376]\n",
      "[Epoch: 27 | Training loss: 0.4375897943973541]\n",
      "[Epoch: 27 | Validation loss: 0.37259241938591003]\n",
      "[Time elapsed for epoch: 8.108338832855225 seconds.] \n",
      "\n",
      "[Epoch: 28 | Batch: 0 | Loss: 0.4370]\n",
      "[Epoch: 28 | Training loss: 0.4369845688343048]\n",
      "[Epoch: 28 | Validation loss: 0.3746022582054138]\n",
      "[Time elapsed for epoch: 8.417397022247314 seconds.] \n",
      "\n",
      "[Epoch: 29 | Batch: 0 | Loss: 0.4377]\n",
      "[Epoch: 29 | Training loss: 0.4376504719257355]\n",
      "[Epoch: 29 | Validation loss: 0.37255921959877014]\n",
      "[Time elapsed for epoch: 8.2510347366333 seconds.] \n",
      "\n",
      "[Epoch: 30 | Batch: 0 | Loss: 0.4387]\n",
      "[Epoch: 30 | Training loss: 0.43871861696243286]\n",
      "[Epoch: 30 | Validation loss: 0.3728339970111847]\n",
      "[Time elapsed for epoch: 7.747149229049683 seconds.] \n",
      "\n",
      "[Epoch: 31 | Batch: 0 | Loss: 0.4363]\n",
      "[Epoch: 31 | Training loss: 0.4363035559654236]\n",
      "[Epoch: 31 | Validation loss: 0.3763621747493744]\n",
      "[Time elapsed for epoch: 7.593736886978149 seconds.] \n",
      "\n",
      "[Epoch: 32 | Batch: 0 | Loss: 0.4384]\n",
      "[Epoch: 32 | Training loss: 0.4384327232837677]\n",
      "[Epoch: 32 | Validation loss: 0.37245091795921326]\n",
      "[Time elapsed for epoch: 7.370280027389526 seconds.] \n",
      "\n",
      "[Epoch: 33 | Batch: 0 | Loss: 0.4361]\n",
      "[Epoch: 33 | Training loss: 0.43612784147262573]\n",
      "[Epoch: 33 | Validation loss: 0.37229910492897034]\n",
      "[Time elapsed for epoch: 7.759475231170654 seconds.] \n",
      "\n",
      "[Epoch: 34 | Batch: 0 | Loss: 0.4371]\n",
      "[Epoch: 34 | Training loss: 0.43712088465690613]\n",
      "[Epoch: 34 | Validation loss: 0.37350961565971375]\n",
      "[Time elapsed for epoch: 8.389668941497803 seconds.] \n",
      "\n",
      "[Epoch: 35 | Batch: 0 | Loss: 0.4366]\n",
      "[Epoch: 35 | Training loss: 0.4365655183792114]\n",
      "[Epoch: 35 | Validation loss: 0.37383678555488586]\n",
      "[Time elapsed for epoch: 8.332012176513672 seconds.] \n",
      "\n",
      "[Epoch: 36 | Batch: 0 | Loss: 0.4366]\n",
      "[Epoch: 36 | Training loss: 0.43656882643699646]\n",
      "[Epoch: 36 | Validation loss: 0.3720160722732544]\n",
      "[Time elapsed for epoch: 8.41195273399353 seconds.] \n",
      "\n",
      "[Epoch: 37 | Batch: 0 | Loss: 0.4355]\n",
      "[Epoch: 37 | Training loss: 0.4355269968509674]\n",
      "[Epoch: 37 | Validation loss: 0.3726695775985718]\n",
      "[Time elapsed for epoch: 8.33678674697876 seconds.] \n",
      "\n",
      "[Epoch: 38 | Batch: 0 | Loss: 0.4362]\n",
      "[Epoch: 38 | Training loss: 0.436174213886261]\n",
      "[Epoch: 38 | Validation loss: 0.3737128674983978]\n",
      "[Time elapsed for epoch: 7.919842004776001 seconds.] \n",
      "\n",
      "[Epoch: 39 | Batch: 0 | Loss: 0.4351]\n",
      "[Epoch: 39 | Training loss: 0.43508246541023254]\n",
      "[Epoch: 39 | Validation loss: 0.3739033341407776]\n",
      "[Time elapsed for epoch: 7.617763042449951 seconds.] \n",
      "\n",
      "[Epoch: 40 | Batch: 0 | Loss: 0.4348]\n",
      "[Epoch: 40 | Training loss: 0.43483632802963257]\n",
      "[Epoch: 40 | Validation loss: 0.37244468927383423]\n",
      "[Time elapsed for epoch: 7.745245933532715 seconds.] \n",
      "\n",
      "[Epoch: 41 | Batch: 0 | Loss: 0.4352]\n",
      "[Epoch: 41 | Training loss: 0.4352364242076874]\n",
      "[Epoch: 41 | Validation loss: 0.3745191693305969]\n",
      "[Time elapsed for epoch: 7.943415641784668 seconds.] \n",
      "\n",
      "[Epoch: 42 | Batch: 0 | Loss: 0.4347]\n",
      "[Epoch: 42 | Training loss: 0.4346930980682373]\n",
      "[Epoch: 42 | Validation loss: 0.3738044202327728]\n",
      "[Time elapsed for epoch: 7.898087024688721 seconds.] \n",
      "\n",
      "[Epoch: 43 | Batch: 0 | Loss: 0.4338]\n",
      "[Epoch: 43 | Training loss: 0.43379563093185425]\n",
      "[Epoch: 43 | Validation loss: 0.3732447624206543]\n",
      "[Time elapsed for epoch: 8.00724196434021 seconds.] \n",
      "\n",
      "[Epoch: 44 | Batch: 0 | Loss: 0.4342]\n",
      "[Epoch: 44 | Training loss: 0.4342197775840759]\n",
      "[Epoch: 44 | Validation loss: 0.3756580948829651]\n",
      "[Time elapsed for epoch: 8.533989191055298 seconds.] \n",
      "\n",
      "[Epoch: 45 | Batch: 0 | Loss: 0.4342]\n",
      "[Epoch: 45 | Training loss: 0.4341752827167511]\n",
      "[Epoch: 45 | Validation loss: 0.3737136125564575]\n",
      "[Time elapsed for epoch: 8.192318201065063 seconds.] \n",
      "\n",
      "[Epoch: 46 | Batch: 0 | Loss: 0.4335]\n",
      "[Epoch: 46 | Training loss: 0.43345358967781067]\n",
      "[Epoch: 46 | Validation loss: 0.373220294713974]\n",
      "[Time elapsed for epoch: 8.365148067474365 seconds.] \n",
      "\n",
      "[Epoch: 47 | Batch: 0 | Loss: 0.4338]\n",
      "[Epoch: 47 | Training loss: 0.4337872862815857]\n",
      "[Epoch: 47 | Validation loss: 0.37739309668540955]\n",
      "[Time elapsed for epoch: 7.628591060638428 seconds.] \n",
      "\n",
      "[Epoch: 48 | Batch: 0 | Loss: 0.4348]\n",
      "[Epoch: 48 | Training loss: 0.43478184938430786]\n",
      "[Epoch: 48 | Validation loss: 0.37544530630111694]\n",
      "[Time elapsed for epoch: 7.576025009155273 seconds.] \n",
      "\n",
      "[Epoch: 49 | Batch: 0 | Loss: 0.4329]\n",
      "[Epoch: 49 | Training loss: 0.432935506105423]\n",
      "[Epoch: 49 | Validation loss: 0.374299019575119]\n",
      "[Time elapsed for epoch: 8.191793203353882 seconds.] \n",
      "\n",
      "[Epoch: 50 | Batch: 0 | Loss: 0.4352]\n",
      "[Epoch: 50 | Training loss: 0.4352462887763977]\n",
      "[Epoch: 50 | Validation loss: 0.3749534487724304]\n",
      "[Time elapsed for epoch: 7.943413019180298 seconds.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "print(f\"[Started training at: {datetime.datetime.now()}. Training for {epochs} epochs.]\")\n",
    "print(f\"[Starting epoch: {start_epoch}]\")\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    start = time.time()\n",
    "    train_loss = 0\n",
    "    validation_loss = 0\n",
    "\n",
    "    # Training loop\n",
    "    train_batches = 0\n",
    "    for (batch, (img_tensor, target)) in enumerate(train_dataset):\n",
    "        batch_loss, sequence_loss = train_step(img_tensor, target)\n",
    "        train_loss += sequence_loss\n",
    "        train_batches = batch\n",
    "        if batch % 50 == 0:\n",
    "            print(f\"[Epoch: {epoch + 1} | Batch: {batch} | Loss: {sequence_loss:.4f}]\")\n",
    "\n",
    "    # Print training loss before starting validation loop\n",
    "    print(f\"[Epoch: {epoch + 1} | Training loss: {train_loss / (train_batches + 1)}]\")\n",
    "    \n",
    "    # Validation loop   \n",
    "    validation_batches = 0\n",
    "    for (batch, (img_tensor, target)) in enumerate(validation_dataset):\n",
    "        batch_loss, sequence_loss = validate_step(img_tensor, target)\n",
    "        validation_loss += sequence_loss    \n",
    "        validation_batches = batch\n",
    "\n",
    "    print(f\"[Epoch: {epoch + 1} | Validation loss: {validation_loss / (validation_batches + 1)}]\")\n",
    "    \n",
    "    # Save epoch losses\n",
    "    train_epoch_losses.append(train_loss / (train_batches + 1))\n",
    "    validation_epoch_losses.append(validation_loss / (validation_batches + 1))\n",
    "\n",
    "    # Save checkpoint (if required)\n",
    "    if epoch % save_at_epoch == 0:\n",
    "        checkpoint_manager.save()\n",
    "\n",
    "    print(f\"[Time elapsed for epoch: {format(time.time() - start)} seconds.] \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Below, we plot some examples taken from the test set, make a prediction and attempt to render the latex back into an image.\n",
    "Subjectively, we find that of the latex is invalid; a bracket is missing, or a character is in an invalid position. \n",
    "Nonetheless, when trained with enough data, the predictions often have significant overlap with the ground truth label. This\n",
    "is the nature of deep learning models -- without a set of rules to govern the output, it all must be learned from data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: id=430131, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=430251, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=430284, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=430317, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=430350, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=430383, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=430496, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=430529, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=430571, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=430684, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=430717, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=430750, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=430783, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=430816, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=430929, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=430962, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=430995, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=431028, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=431061, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=431174, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=431207, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=431240, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=431273, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=431306, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=431419, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=431452, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=431485, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=431518, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=431551, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=431664, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=431697, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=431730, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=431763, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=431796, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=431909, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=431942, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=431975, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=432008, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=432041, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=432154, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=432187, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=432220, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=432253, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=432286, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=432399, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=432432, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=432465, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=432498, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=432531, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=432644, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=432677, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=432710, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=432743, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=432776, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=432889, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=432922, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=432955, shape=(), dtype=float32, numpy=inf>,\n",
       " <tf.Tensor: id=432988, shape=(), dtype=float32, numpy=inf>]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_epoch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_epoch_losses)\n",
    "plt.plot(validation_epoch_losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.legend([\"Training loss\", \"Validation loss\"])\n",
    "plt.savefig(f\"{figs_base_dir}/{datetime.datetime.now()}.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_image(path):\n",
    "    img_raw = tf.io.read_file(path)\n",
    "    return tf.image.decode_png(img_raw) \n",
    "\n",
    "\n",
    "def plot_features(features):\n",
    "    layer_count = features.shape[3]\n",
    "    \n",
    "    num_rows = 16\n",
    "    num_cols = int(layer_count / num_rows)\n",
    "    \n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(32, 32))\n",
    "    for i, ax in enumerate(axes.flat):        \n",
    "        feat = features[0,:,:,i]\n",
    "        ax.imshow(feat)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_attention(attn):\n",
    "    layer_count = attn.shape[0]\n",
    "    \n",
    "    num_rows = 16\n",
    "    num_cols = math.floor(layer_count / num_rows)\n",
    "    \n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(32, 20))\n",
    "    for i, ax in enumerate(axes.flat):       \n",
    "        if i >= layer_count:\n",
    "            break\n",
    "        feat = attn[i,:,:]\n",
    "        ax.imshow(feat)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def evaluate(img, max_formula_length):\n",
    "    index_token_mapping = {v: k for k, v in vocab.token_index.items()} \n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "    # Convert to grayscale so network can process it\n",
    "    # and add a dimension at the start to simulate batch (size=1)\n",
    "    image = tf.image.rgb_to_grayscale(img)\n",
    "    temp_input = tf.expand_dims(image, 0)\n",
    "\n",
    "    # Pass through encoder to obtain features\n",
    "    features = encoder(temp_input)\n",
    "\n",
    "    # Signal to the decoder that we are starting feeding in a new sequence\n",
    "    decoder_input = tf.expand_dims(vocab.tokenize_formula(vocab.start), 1)\n",
    "            \n",
    "    result = []\n",
    "    attention_plot = np.zeros((max_formula_length, features.shape[1], features.shape[2]))\n",
    "    for i in range(max_formula_length):\n",
    "        predictions, hidden, attention_weights = decoder(decoder_input, features, hidden)\n",
    "        \n",
    "        #attn weights: (1, feat_width * feat_height, 1)\n",
    "        attention_plot[i] = tf.reshape(attention_weights, (features.shape[1], features.shape[2])).numpy()    \n",
    "       \n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        result.append(index_token_mapping[predicted_id])\n",
    "    \n",
    "        if index_token_mapping[predicted_id] == vocab.end:\n",
    "            # Strip end token when returning\n",
    "            return result[:-1], attention_plot, features\n",
    "        \n",
    "        decoder_input = tf.expand_dims([predicted_id], 0)\n",
    "    return result, attention_plot, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def create_from_template(latex):\n",
    "    '''Uses a simple template to create a valid latex document'''\n",
    "    pre = \"\"\"\n",
    "    \\documentclass[preview]{standalone}\n",
    "    \\\\begin{document}\n",
    "    \\\\begin{equation} \n",
    "    \"\"\"\n",
    "    \n",
    "    post = \"\"\"\n",
    "    \\end{equation}\n",
    "    \\end{document}\n",
    "    \"\"\"\n",
    "    return pre + latex + post\n",
    "    \n",
    "\n",
    "def render_latex(latex):\n",
    "    ''' Renders a latex string, creating a png'''\n",
    "    # Write temp tex file\n",
    "    latex_out_dir = f\"{processed_data_path}latex/\"\n",
    "    temp_tex_filepath = f\"{processed_data_path}latex/tmp.tex\"\n",
    "    temp_dvi_filepath = os.path.splitext(temp_tex_filepath)[0]+'.dvi'\n",
    "    temp_png_filepath = os.path.splitext(temp_tex_filepath)[0]+'1.png'\n",
    "\n",
    "    with open(temp_tex_filepath, 'w+') as f:\n",
    "        f.seek(0)\n",
    "        f.write(create_from_template(latex))\n",
    "    \n",
    "    # Render it\n",
    "    cmd = ['latex', '-interaction=nonstopmode', '--halt-on-error', \"-output-directory\", latex_out_dir, temp_tex_filepath]\n",
    "    output = subprocess.run(cmd, capture_output=True)\n",
    "    if output.returncode != 0:                \n",
    "        print(f\"Could not render latex (status code={output.returncode}. Output follows: \\n\")\n",
    "        print(output.stdout)\n",
    "        print(output.stderr)\n",
    "        print(\"\\n\")\n",
    "        return None\n",
    "    \n",
    "    render_png_cmd = ['dvipng','-D','300', \"-o\", temp_png_filepath, temp_dvi_filepath]\n",
    "    output = subprocess.run(render_png_cmd, capture_output=True)\n",
    "    if output.returncode != 0:\n",
    "        print(\"Could not render .dvi file into .pdf! Output follows: \\n\")\n",
    "        print(output.stdout)\n",
    "        print(output.stderr)\n",
    "        print(\"\\n\")\n",
    "        return None\n",
    "    return temp_png_filepath\n",
    "\n",
    "def plot_image_clean(png):\n",
    "    ''' plots an image without axes'''\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    ax = fig.add_axes([0, 0, 1, 1])\n",
    "    ax.imshow(plt.imread(out_file))\n",
    "    ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def process_latex(latex):\n",
    "    return \" \".join(latex)\n",
    "    \n",
    "# Example params\n",
    "test_index = 5\n",
    "test_img = load_image(validation_images[test_index])\n",
    "\n",
    "# Make prediction\n",
    "result, attention_plot, features = evaluate(test_img, max_formula_length)\n",
    "\n",
    "# Plot original image\n",
    "plt.imshow(test_img)\n",
    "plt.show()\n",
    "\n",
    "# Render latex from the predicted formula\n",
    "latex = process_latex(result)\n",
    "out_file = render_latex(latex)\n",
    "if out_file is not None:\n",
    "    plot_image_clean(out_file)\n",
    "    \n",
    "print(f\"Predicted formula: \\n {result}\")\n",
    "plot_attention(attention_plot)\n",
    "plot_features(features[:,:,:,0:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_for_valid_predictions(count, example_count, train_image_path = f\"{processed_data_path}images/train/\"):\n",
    "    ''' Search for predictions with latex that compiles'''\n",
    "    success_count = 0\n",
    "    index = 0\n",
    "    success_indexes = []\n",
    "    while success_count < count:\n",
    "        \n",
    "        if index >= example_count:\n",
    "            return success_indexes\n",
    "        \n",
    "        # predict\n",
    "        test_img = load_image(f\"{train_image_path}/{index}.png\")\n",
    "        result, attention_plot, features = evaluate(test_img, max_formula_length)\n",
    "        \n",
    "        # attemp to render\n",
    "        latex = process_latex(result)\n",
    "        out_file = render_latex(latex)\n",
    "        \n",
    "        if out_file is not None:\n",
    "            success_indexes.append(index)\n",
    "        \n",
    "        index += 1        \n",
    "        \n",
    "search_for_valid_predictions(10, 200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
