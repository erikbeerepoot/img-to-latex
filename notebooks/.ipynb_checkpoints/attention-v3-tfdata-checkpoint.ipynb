{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "### Make sure our data is in order\n",
    "data_base_dir = \"../data\"\n",
    "figs_base_dir = \"../figs\"\n",
    "\n",
    "original_data_path = data_base_dir + \"/original/formula/\"\n",
    "processed_data_path = data_base_dir + \"/processed/formula/\"\n",
    "pickle_data_path = data_base_dir + \"/pickle/formula/\"\n",
    "\n",
    "assert os.path.exists(original_data_path), \"Original data path does not exist.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images = glob.glob(f\"{processed_data_path}/images/train/*.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 76322 training labels.\n",
      "Found 76303 training matches.\n",
      "Kept 76303 training labels.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "# formulas = pd.read_csv(f\"{processed_data_path}formulas.norm.lst\", sep='~!!!~~~', header=None)\n",
    "with open(f\"{processed_data_path}labels/train.formulas.norm.txt\") as f:\n",
    "    train_labels = np.array(f.read().splitlines())\n",
    "    \n",
    "train_matches = pd.read_csv(f\"{processed_data_path}images/train/train.matching.txt\", sep=' ', header=None).values\n",
    "\n",
    "print(f\"Found {len(train_labels)} training labels.\")\n",
    "print(f\"Found {len(train_matches)} training matches.\")\n",
    "\n",
    "# Get correct labels\n",
    "train_labels = train_labels[[list(map(lambda f: f[1], train_matches))][0]]\n",
    "\n",
    "print(f\"Kept {len(train_labels)} training labels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 76303 training images.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Found {len(training_images)} training images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "    def __init__(self, vocab_path):\n",
    "        self.build_vocab(vocab_path)\n",
    "        \n",
    "    def build_vocab(self, vocab_path):\n",
    "        '''\n",
    "        Builds the complete vocabulary, including special tokens\n",
    "        '''\n",
    "        self.unk   = \"<UNK>\"\n",
    "        self.start = \"<SOS>\"\n",
    "        self.end   = \"<END>\"\n",
    "        self.pad   = \"<PAD>\"\n",
    "        \n",
    "        # First, load our vocab from disk & determine \n",
    "        # highest index in mapping.\n",
    "        vocab = self.load_vocab(vocab_path)\n",
    "        max_index = max(vocab.values())\n",
    "        \n",
    "        # Compile special token mapping\n",
    "        special_tokens = {\n",
    "            self.unk : max_index + 1,\n",
    "            self.start : max_index + 2,\n",
    "            self.end : max_index + 3,\n",
    "            self.pad : max_index + 4\n",
    "        }\n",
    "        \n",
    "        # Merge dicts to produce final word index\n",
    "        self.token_index = {**vocab, **special_tokens}\n",
    "        self.reverse_index = {v: k for k, v in self.token_index.items()} \n",
    "    \n",
    "    def load_vocab(self, vocab_path):\n",
    "        '''\n",
    "        Load vocabulary from file\n",
    "        '''\n",
    "        token_index = {}\n",
    "        with open(vocab_path) as f:\n",
    "            for idx, token in enumerate(f):\n",
    "                token = token.strip()\n",
    "                token_index[token] = idx\n",
    "        assert len(token_index) > 0, \"Could not build word index\"\n",
    "        return token_index\n",
    "                \n",
    "    def tokenize_formula(self, formula):\n",
    "        '''\n",
    "        Converts a formula into a sequence of tokens using the vocabulary\n",
    "        '''\n",
    "        def lookup_token(token):\n",
    "            return self.token_index[token] if token in self.token_index else self.token_index[self.unk]\n",
    "        tokens = formula.strip().split(' ')        \n",
    "        return list(map(lambda f: lookup_token(f), tokens))\n",
    "        \n",
    "    def pad_formula(self, formula, max_length):\n",
    "        '''\n",
    "        Pads a formula to max_length with pad_token, appending end_token.\n",
    "        '''\n",
    "        # Extra space for the end token\n",
    "        padded_formula = self.token_index[self.pad] * np.ones(max_length + 1)\n",
    "        padded_formula[len(formula)] = self.token_index[self.end]\n",
    "        padded_formula[:len(formula)] = formula\n",
    "        return padded_formula\n",
    "\n",
    "    @property\n",
    "    def length(self):\n",
    "        return len(self.token_index)\n",
    "    \n",
    "vocab = Vocab(f\"{processed_data_path}/vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "buffer_size = 1000\n",
    "batch_size = 16\n",
    "embedding_dim = 256\n",
    "vocab_size = vocab.length\n",
    "hidden_units = 256\n",
    "num_datapoints = 16384\n",
    "num_steps = num_datapoints // batch_size\n",
    "epochs = 30 \n",
    "train_new_model = True\n",
    "max_image_size=(50,200)\n",
    "max_formula_length = 130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This hash table is used to perform token lookups in the vocab\n",
    "table = tf.lookup.StaticHashTable(\n",
    "    initializer=tf.lookup.KeyValueTensorInitializer(\n",
    "        keys=tf.constant(list(vocab.token_index.keys())),\n",
    "        values=tf.constant(list(vocab.token_index.values())),\n",
    "    ),\n",
    "    default_value=tf.constant(vocab.token_index[vocab.unk]),\n",
    "    name=\"class_weight\"\n",
    ")\n",
    "\n",
    "def load_and_decode_img(path):\n",
    "    ''' Load the image and decode from png'''\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_png(image)\n",
    "    return tf.image.rgb_to_grayscale(image)\n",
    "\n",
    "@tf.function\n",
    "def lookup_token(token):\n",
    "    ''' Lookup the given token in the vocab'''\n",
    "    table.lookup(token)\n",
    "    return  table.lookup(token)\n",
    "\n",
    "def process_label(label):\n",
    "    ''' Split to tokens, lookup & append <END> token'''\n",
    "    tokens = tf.strings.split(label, \" \")   \n",
    "    tokens = tf.map_fn(lookup_token, tokens, dtype=tf.int32)\n",
    "    return tf.concat([tokens, [vocab.token_index[vocab.end]]], 0)\n",
    "\n",
    "def process_datum(path, label):\n",
    "    return load_and_decode_img(path), process_label(label)\n",
    "\n",
    "# Tokenize formulas\n",
    "dataset = tf.data.Dataset.from_tensor_slices((training_images, train_labels)).map(process_datum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This hash table is used to perform token lookups in the vocab\n",
    "table = tf.lookup.StaticHashTable(\n",
    "    initializer=tf.lookup.KeyValueTensorInitializer(\n",
    "        keys=tf.constant(list(vocab.token_index.keys())),\n",
    "        values=tf.constant(list(vocab.token_index.values())),\n",
    "    ),\n",
    "    default_value=tf.constant(vocab.token_index[vocab.unk]),\n",
    "    name=\"class_weight\"\n",
    ")\n",
    "\n",
    "def load_and_decode_img(path):\n",
    "    ''' Load the image and decode from png'''\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_png(image)\n",
    "    return tf.image.rgb_to_grayscale(image)\n",
    "\n",
    "@tf.function\n",
    "def lookup_token(token):\n",
    "    ''' Lookup the given token in the vocab'''\n",
    "    table.lookup(token)\n",
    "    return  table.lookup(token)\n",
    "\n",
    "def process_label(label):\n",
    "    ''' Split to tokens, lookup & append <END> token'''\n",
    "    tokens = tf.strings.split(label, \" \")   \n",
    "    return tokens\n",
    "\n",
    "def process_datum(path, label):\n",
    "    return load_and_decode_img(path), process_label(label)\n",
    "\n",
    "# Tokenize formulas\n",
    "dataset = tf.data.Dataset.from_tensor_slices((training_images, train_labels)).map(process_datum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[480 498 473 507  21 509  35   4  20   9 507 213 507 496 478 493 498 428\n",
      " 509 507 497 509 509   5 473 507 213 507  21 509 507  20   7 121 473 507\n",
      "  21 509 509 509 248 480 497 473 507  21 509   7 497 473 507  21 509 480\n",
      " 428 473 507  21 509   7 497 473 507  21 509 498 485 492 473 507  21 509\n",
      " 428 480 446 473 507  21 509 355   9 507 213 507 480 499 473 507  21 509\n",
      " 509 507   4  20   9 507 213 507 496 478 493 498 428 509 507 497 509 509\n",
      "   5 473 507 213 507  21 509 507  20   7 121 473 507  21 509 509 509 509\n",
      " 509  74  12 514], shape=(130,), dtype=int32)\n",
      "\n",
      "\n",
      "tf.Tensor(\n",
      "[465 215 474 507 290 507 484 493 494 482 509 509 392 415 474 507 492  36\n",
      "  14 509 465 507  45 509 474 507 492 509 507 213 507   4   9 476   5 473\n",
      " 507 492 509 509 507  21 473 507  21 492   9  20 509 509 509 514], shape=(52,), dtype=int32)\n",
      "\n",
      "\n",
      "tf.Tensor(\n",
      "[  4 507 162  50 509 474 507 476 509 483   5 474 507 485 487 509  35  14\n",
      "   8  68  68  68  68   4 507 162  50 509 474 507 476 509  46   5 474 507\n",
      " 485 487 488 509  35  14   8 514], shape=(44,), dtype=int32)\n",
      "\n",
      "\n",
      "tf.Tensor(\n",
      "[ 58 474 507 498 499 476 499 509  35  21 337 406 507  52 474 507  26 509\n",
      " 473 507   4  20   5 509  52 474 507  26 509 473 507   4  21   5 509  52\n",
      " 474 507  26 509 473 507   4  22   5 509 509 253 406 507 492 509   7 406\n",
      " 507 131 507 492 509 509 363 514], shape=(62,), dtype=int32)\n",
      "\n",
      "\n",
      "tf.Tensor(\n",
      "[220 507  52 509 474 507  22 509  35 415 401 482 474 507 487  35  20 509\n",
      " 476 474 507 487 509 401 507 178 509 476 474 507 487 509  74  12 514], shape=(35,), dtype=int32)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print some values from the dataset (pre-filter)\n",
    "for datum in dataset.take(5):\n",
    "    print(datum[1])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_size(image, label):\n",
    "    '''Filter the dataset by the size of the image & length of label'''\n",
    "    label_length = tf.shape(label)\n",
    "    image_size = tf.shape(image)\n",
    "    \n",
    "    # Does this image meet our size constraint?\n",
    "    keep_image = tf.math.reduce_all(\n",
    "        tf.math.greater_equal(max_image_size, image_size[:2])\n",
    "    )\n",
    "    # Does this image meet our formula length constraint?\n",
    "    keep_label = tf.math.reduce_all(\n",
    "        tf.math.greater_equal(max_formula_length, label_length[0])\n",
    "    )\n",
    "    return tf.math.logical_and(keep_image, keep_label)\n",
    "\n",
    "data = dataset.filter(filter_by_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[480 498 473 507  21 509  35   4  20   9 507 213 507 496 478 493 498 428\n",
      " 509 507 497 509 509   5 473 507 213 507  21 509 507  20   7 121 473 507\n",
      "  21 509 509 509 248 480 497 473 507  21 509   7 497 473 507  21 509 480\n",
      " 428 473 507  21 509   7 497 473 507  21 509 498 485 492 473 507  21 509\n",
      " 428 480 446 473 507  21 509 355   9 507 213 507 480 499 473 507  21 509\n",
      " 509 507   4  20   9 507 213 507 496 478 493 498 428 509 507 497 509 509\n",
      "   5 473 507 213 507  21 509 507  20   7 121 473 507  21 509 509 509 509\n",
      " 509  74 512 514], shape=(130,), dtype=int32)\n",
      "\n",
      "\n",
      "tf.Tensor(\n",
      "[483 474 507 485 487 509   4 504   5  35 507 213 507  20 509 507 476 473\n",
      " 507  21 509 509 509  74 183 474 507 485 487 509   8 510 510 336 473 507\n",
      " 476 509   4 504   5  35 336 473 507 476 509   8 351   4 476   8 336 473\n",
      " 507 476 509  69  32 510 290 507 478 493 492 498 499  12 509 512 514], shape=(71,), dtype=int32)\n",
      "\n",
      "\n",
      "tf.Tensor(\n",
      "[362 474 507  50 509   4 496   5  35 415 474 507 490  35  20 509 473 507\n",
      "  50 509  68  55 474 507  50 509   4 490   5  68 507 213 507  20 509 507\n",
      " 496 473 507 490   9  20 509 509 509  68  68 512 514], shape=(49,), dtype=int32)\n",
      "\n",
      "\n",
      "tf.Tensor(\n",
      "[348   4 215   5  35 322 507 481 504 494 509 507   9   4 507 425 507 213\n",
      " 507 483 473 507  21 509 509 507  21 509 509 509   5 234 474 507 215 509\n",
      " 480 505 473 507 476 509 234 474 507 215 509 480 505 473 507 476 473 507\n",
      " 343 509 509  42 474 507  20 509   4 505   9 505 473 507 343 509   5 512\n",
      " 514], shape=(73,), dtype=int32)\n",
      "\n",
      "\n",
      "tf.Tensor(\n",
      "[246  59 474 507 506 506 509 354  35   9  22 432 213 507 337 473 507  21\n",
      " 509 509 507  20  25  25  14 476 473 507  25 509 509 512 514], shape=(33,), dtype=int32)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for datum in data.take(5):\n",
    "    print(datum[1])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and batch (dropping any batches that are < BATCH_SIZE long)\n",
    "# also pad each batch to the largest image size + formulas to\n",
    "# max_formula_length.\n",
    "shapes = (tf.TensorShape([None,None,1]),tf.TensorShape([max_formula_length]))\n",
    "values = (tf.constant(255, dtype=tf.uint8), tf.constant(vocab.token_index[vocab.pad]))\n",
    "dataset = data.shuffle(buffer_size).padded_batch(\n",
    "    batch_size, \n",
    "    padded_shapes=shapes,\n",
    "    padding_values=values,\n",
    "    drop_remainder=True\n",
    ")\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import math\n",
    "import numpy as np\n",
    "from six.moves import xrange\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# taken from https://github.com/tensorflow/tensor2tensor/blob/37465a1759e278e8f073cd04cd9b4fe377d3c740/tensor2tensor/layers/common_attention.py\n",
    "def add_timing_signal_nd(x, min_timescale=1.0, max_timescale=1.0e4):\n",
    "    \"\"\"Adds a bunch of sinusoids of different frequencies to a Tensor.\n",
    "\n",
    "    Each channel of the input Tensor is incremented by a sinusoid of a difft\n",
    "    frequency and phase in one of the positional dimensions.\n",
    "\n",
    "    This allows attention to learn to use absolute and relative positions.\n",
    "    Timing signals should be added to some precursors of both the query and the\n",
    "    memory inputs to attention.\n",
    "\n",
    "    The use of relative position is possible because sin(a+b) and cos(a+b) can\n",
    "    be expressed in terms of b, sin(a) and cos(a).\n",
    "\n",
    "    x is a Tensor with n \"positional\" dimensions, e.g. one dimension for a\n",
    "    sequence or two dimensions for an image\n",
    "\n",
    "    We use a geometric sequence of timescales starting with\n",
    "    min_timescale and ending with max_timescale.  The number of different\n",
    "    timescales is equal to channels // (n * 2). For each timescale, we\n",
    "    generate the two sinusoidal signals sin(timestep/timescale) and\n",
    "    cos(timestep/timescale).  All of these sinusoids are concatenated in\n",
    "    the channels dimension.\n",
    "\n",
    "    Args:\n",
    "        x: a Tensor with shape [batch, d1 ... dn, channels]\n",
    "        min_timescale: a float\n",
    "        max_timescale: a float\n",
    "\n",
    "    Returns:\n",
    "        a Tensor the same shape as x.\n",
    "\n",
    "    \"\"\"\n",
    "    static_shape = x.get_shape().as_list()\n",
    "    num_dims = len(static_shape) - 2\n",
    "    channels = tf.shape(x)[-1]\n",
    "    num_timescales = channels // (num_dims * 2)\n",
    "    log_timescale_increment = (\n",
    "            math.log(float(max_timescale) / float(min_timescale)) /\n",
    "            (tf.cast(num_timescales, tf.float32) - 1))\n",
    "    inv_timescales = min_timescale * tf.exp(\n",
    "            tf.cast(tf.range(num_timescales), tf.float32) * -log_timescale_increment)\n",
    "    for dim in xrange(num_dims):\n",
    "        length = tf.shape(x)[dim + 1]\n",
    "        position = tf.cast(tf.range(length), tf.float32)\n",
    "        scaled_time = tf.expand_dims(position, 1) * tf.expand_dims(\n",
    "                inv_timescales, 0)\n",
    "        signal = tf.concat([tf.sin(scaled_time), tf.cos(scaled_time)], axis=1)\n",
    "        prepad = dim * 2 * num_timescales\n",
    "        postpad = channels - (dim + 1) * 2 * num_timescales\n",
    "        signal = tf.pad(signal, [[0, 0], [prepad, postpad]])\n",
    "        for _ in xrange(1 + dim):\n",
    "            signal = tf.expand_dims(signal, 0)\n",
    "        for _ in xrange(num_dims - 1 - dim):\n",
    "            signal = tf.expand_dims(signal, -2)\n",
    "        x += signal\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import metrics, layers, Model\n",
    "\n",
    "class BahdanauAttention(layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, features, hidden):\n",
    "        # First, flatten the image\n",
    "        shape = tf.shape(features)\n",
    "        if len(shape) == 4:\n",
    "            batch_size = shape[0]\n",
    "            img_height = shape[1]\n",
    "            img_width  = shape[2]\n",
    "            channels   = shape[3]\n",
    "            features = tf.reshape(features, shape=(batch_size, img_height*img_width, channels))\n",
    "        else:\n",
    "            print(f\"Image shape not supported: {shape}.\")\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        # features(CNN_encoder_output)\n",
    "        # shape => (batch_size, flattened_image_size, embedding_size)\n",
    "        \n",
    "        # hidden shape == (batch_size, hidden_size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        # score \n",
    "        # shape => (batch_size, flattened_image_size, hidden_size)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "        # attention_weights\n",
    "        # shape => (batch_size, flattened_image_size, 1)\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        \n",
    "        # context_vector\n",
    "        # shape after sum => (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        return context_vector, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNEncoder(tf.keras.Model):    \n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNNEncoder, self).__init__()\n",
    "        \n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "        \n",
    "    def build(self, input_shape):       \n",
    "        self.cnn_1 = layers.Conv2D(64, (3, 3), activation='relu', input_shape=(input_shape[1], input_shape[2], 1))\n",
    "        self.max_pool_1 = layers.MaxPooling2D((2, 2))\n",
    "        \n",
    "        self.cnn_2 = layers.Conv2D(256, (3, 3), activation='relu')\n",
    "        self.max_pool_2 = layers.MaxPooling2D((2, 2))\n",
    "        \n",
    "        self.cnn_3 = layers.Conv2D(512, (3, 3), activation='relu')\n",
    "        \n",
    "    def call(self, images):\n",
    "        images = tf.cast(images, tf.float32)\n",
    "        x = self.cnn_1(images)\n",
    "        x = self.max_pool_1(x)\n",
    "        x = self.cnn_2(x)\n",
    "        x = self.max_pool_2(x)\n",
    "        x = self.cnn_3(x)\n",
    "        return add_timing_signal_nd(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNDecoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNNDecoder, self).__init__()\n",
    "        self.units = units\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        \n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "        \n",
    "    def call(self, x, features, hidden):        \n",
    "        # attend over the image features\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "        \n",
    "        # convert our input vector to an embedding\n",
    "        x = self.embedding(x)\n",
    "                \n",
    "        # concat the embedding and the context vector (from attention)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # pass to GRU\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        x = self.fc1(output)\n",
    "        \n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "        \n",
    "        # This produces a distribution over the vocab\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x, state, attention_weights\n",
    "    \n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNNEncoder(embedding_dim=embedding_dim)\n",
    "decoder = RNNDecoder(embedding_dim, hidden_units, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    # In order to avoid <PAD> tokens contributing to the loss, we mask those tokens.\n",
    "    # First, we create the mask, and compute the loss.\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, vocab.token_index[vocab.pad]))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    # Second, we multiply the computed loss by the mask to zero out contributions from the <PAD> tokens.\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not restore from a checkpoint -- training new model!\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "checkpoint = tf.train.Checkpoint(encoder=encoder,\n",
    "                                 decoder=decoder,\n",
    "                                 optimizer=optimizer)\n",
    "checkpoint_manager = tf.train.CheckpointManager(checkpoint, checkpoint_path, max_to_keep=10)\n",
    "\n",
    "# Attempt to restore from training checkpoint\n",
    "start_epoch = 0\n",
    "save_at_epoch = 5\n",
    "if train_new_model is False and checkpoint_manager.latest_checkpoint:\n",
    "    checkpoint.restore(checkpoint_manager.latest_checkpoint)\n",
    "    start_epoch = int(checkpoint_manager.latest_checkpoint.split('-')[-1])*save_at_epoch\n",
    "    print(f\"Restored from checkpoint: {checkpoint_manager.latest_checkpoint}.\")\n",
    "    print(f\"Start epoch: {start_epoch}.\")\n",
    "else:\n",
    "    print(\"Did not restore from a checkpoint -- training new model!\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    ''' Function that encapsulates training logic'''\n",
    "    loss = 0\n",
    "\n",
    "    # reset the decoder state, since Latex is different for each image    \n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "    # shape => (batch_size, 1)\n",
    "    dec_input = tf.expand_dims([vocab.token_index[vocab.start]] * batch_size, 1)\n",
    "\n",
    "    sequence_length = target.shape[1]    \n",
    "    with tf.GradientTape() as tape:\n",
    "        features = encoder(img_tensor)\n",
    "\n",
    "        for i in range(0, sequence_length):\n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "            ground_truth_token = target[:, i]\n",
    "            loss += loss_function(ground_truth_token, predictions)\n",
    "                \n",
    "            # Teacher forcing: feed the correct word in as the next input to the\n",
    "            # encoder, to provide the decoder with the proper context to predict\n",
    "            # the following token in the sequence\n",
    "            dec_input = tf.expand_dims(ground_truth_token, 1)\n",
    "\n",
    "    total_loss = (loss / int(sequence_length))\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[480,\n",
       " 498,\n",
       " 473,\n",
       " 507,\n",
       " 21,\n",
       " 509,\n",
       " 35,\n",
       " 4,\n",
       " 20,\n",
       " 9,\n",
       " 507,\n",
       " 213,\n",
       " 507,\n",
       " 496,\n",
       " 478,\n",
       " 493,\n",
       " 498,\n",
       " 428,\n",
       " 509,\n",
       " 507,\n",
       " 497,\n",
       " 509,\n",
       " 509,\n",
       " 5,\n",
       " 473,\n",
       " 507,\n",
       " 213,\n",
       " 507,\n",
       " 21,\n",
       " 509,\n",
       " 507,\n",
       " 20,\n",
       " 7,\n",
       " 121,\n",
       " 473,\n",
       " 507,\n",
       " 21,\n",
       " 509,\n",
       " 509,\n",
       " 509,\n",
       " 248,\n",
       " 480,\n",
       " 497,\n",
       " 473,\n",
       " 507,\n",
       " 21,\n",
       " 509,\n",
       " 7,\n",
       " 497,\n",
       " 473,\n",
       " 507,\n",
       " 21,\n",
       " 509,\n",
       " 480,\n",
       " 428,\n",
       " 473,\n",
       " 507,\n",
       " 21,\n",
       " 509,\n",
       " 7,\n",
       " 497,\n",
       " 473,\n",
       " 507,\n",
       " 21,\n",
       " 509,\n",
       " 498,\n",
       " 485,\n",
       " 492,\n",
       " 473,\n",
       " 507,\n",
       " 21,\n",
       " 509,\n",
       " 428,\n",
       " 480,\n",
       " 446,\n",
       " 473,\n",
       " 507,\n",
       " 21,\n",
       " 509,\n",
       " 355,\n",
       " 9,\n",
       " 507,\n",
       " 213,\n",
       " 507,\n",
       " 480,\n",
       " 499,\n",
       " 473,\n",
       " 507,\n",
       " 21,\n",
       " 509,\n",
       " 509,\n",
       " 507,\n",
       " 4,\n",
       " 20,\n",
       " 9,\n",
       " 507,\n",
       " 213,\n",
       " 507,\n",
       " 496,\n",
       " 478,\n",
       " 493,\n",
       " 498,\n",
       " 428,\n",
       " 509,\n",
       " 507,\n",
       " 497,\n",
       " 509,\n",
       " 509,\n",
       " 5,\n",
       " 473,\n",
       " 507,\n",
       " 213,\n",
       " 507,\n",
       " 21,\n",
       " 509,\n",
       " 507,\n",
       " 20,\n",
       " 7,\n",
       " 121,\n",
       " 473,\n",
       " 507,\n",
       " 21,\n",
       " 509,\n",
       " 509,\n",
       " 509,\n",
       " 509,\n",
       " 509,\n",
       " 74,\n",
       " 12]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.token_index[\".\"]\n",
    "vocab.tokenize_formula(train_labels[0])\n",
    "# for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "#     for label in target:\n",
    "#         print([vocab.reverse_index[token.numpy()] for token in label])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"img_tensor:0\", shape=(16, 50, 200, 1), dtype=uint8)\n",
      "[Epoch: 1 | Batch: 0 | Loss: 6.3455]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-29d36dbfb39e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mbatch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msequence_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml-tf1/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    402\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml-tf1/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml-tf1/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    587\u001b[0m     \"\"\"\n\u001b[1;32m    588\u001b[0m     return self._call_flat(\n\u001b[0;32m--> 589\u001b[0;31m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0m\u001b[1;32m    590\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m    591\u001b[0m                            resource_variable_ops.ResourceVariable))))\n",
      "\u001b[0;32m~/.virtualenvs/ml-tf1/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml-tf1/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    443\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    444\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 445\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    446\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml-tf1/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "print(f\"[Started training at: {datetime.datetime.now()}. Training for {epochs} epochs.]\")\n",
    "print(f\"[Starting epoch: {start_epoch}]\")\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "        batch_loss, sequence_loss = train_step(img_tensor, target)\n",
    "        total_loss += sequence_loss\n",
    "\n",
    "        if batch % 50 == 0:\n",
    "            print(f\"[Epoch: {epoch + 1} | Batch: {batch} | Loss: {sequence_loss:.4f}]\")\n",
    "\n",
    "    # Save epoch loss\n",
    "    epoch_losses.append(total_loss / num_steps)\n",
    "\n",
    "    # Save checkpoint (if required)\n",
    "    if epoch % save_at_epoch == 0:\n",
    "        checkpoint_manager.save()\n",
    "\n",
    "    print(f\"[Epoch: {epoch + 1} | Epoch Loss: {total_loss / num_steps}]\")\n",
    "    print(f\"[Time elapsed for epoch: {format(time.time() - start)} seconds.] \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
