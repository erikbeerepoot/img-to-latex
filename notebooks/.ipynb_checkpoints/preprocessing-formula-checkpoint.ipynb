{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formula recognition using seq2seq models - Data Preprocessing\n",
    "\n",
    "In this notebook, we describe the steps necessary to post process the data and put it in an easy to consume format.\n",
    "\n",
    "The [im2latex dataset](https://zenodo.org/record/56198#.V2p0KTXT6eA) consists of:\n",
    "> [A] total of ~100k formulas and images splitted into train, validation and test sets. Formulas were parsed from LaTeX sources provided here: http://www.cs.cornell.edu/projects/kddcup/datasets.html(originally from  arXiv). Each image is a PNG image of fixed size. Formula is in black and rest of the image is transparent.\n",
    "\n",
    "The image data provided is not terribly useful due to its large size and transparent background. Luckily, a person undertaking a project with\n",
    "this very same topic produced some [helpful code](https://github.com/guillaumegenthial/im2latex). The code needed some trivial fixes to work with python 3. \n",
    "\n",
    "To generate the dataset, run the code below. It renders each formula into a `.png` file, produces a list of formula to file mappings, an generates the vocabulary.\n",
    "\n",
    "**Note:** Generating the full dataset takes several hours, and will run in the background. You will need to manually kill the python process if you wish to interrupt it. If you do this, get a ☕️ and be prepared to wait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom src import\n",
    "import sys \n",
    "sys.path.append('../src/')\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting click\n",
      "  Using cached https://files.pythonhosted.org/packages/fa/37/45185cb5abbc30d7257104c434fe0b07e5a195a6847506c074527aa599ec/Click-7.0-py2.py3-none-any.whl\n",
      "Installing collected packages: click\n",
      "Successfully installed click-7.0\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install click imageio scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 76322 formulas from ../data/original/formula/train.formulas.norm.txt\n",
      "Loaded 9444 formulas from ../data/original/formula/test.formulas.norm.txt\n",
      "Loaded 8475 formulas from ../data/original/formula/val.formulas.norm.txt\n"
     ]
    }
   ],
   "source": [
    "# Adapted from: https://github.com/guillaumegenthial/im2latex\n",
    "\n",
    "import click\n",
    "import json\n",
    "\n",
    "from utils.data_generator import DataGenerator\n",
    "from utils.text import build_vocab, write_vocab\n",
    "from utils.image import build_images\n",
    "from utils.general import Config\n",
    "\n",
    "small_data = json.loads(\"\"\"\n",
    "{\n",
    "    \"export_name\": \"data.json\",\n",
    "\n",
    "    \"dir_images_train\": \"../data/processed/formula/images/train/\",\n",
    "    \"dir_images_test\" : \"../data/processed/formula/images/test/\",\n",
    "    \"dir_images_val\"  : \"../data/processed/formula/images/validation/\",\n",
    "\n",
    "    \"path_matching_train\": \"../data/processed/formula/images/small/train.matching.txt\",\n",
    "    \"path_matching_val\"  : \"../data/processed/formula/images/small/test.matching.txt\",\n",
    "    \"path_matching_test\" : \"../data/processed/formula/images/small/val.matching.txt\",\n",
    "\n",
    "    \"path_formulas_train\": \"../data/original/formula/small.formulas.norm.txt\",\n",
    "    \"path_formulas_test\" : \"../data/original/formula/small.formulas.norm.txt\",\n",
    "    \"path_formulas_val\"  : \"../data/original/formula/small.formulas.norm.txt\",\n",
    "\n",
    "    \"max_iter\"          : 20,\n",
    "    \"max_length_formula\": 50,\n",
    "\n",
    "    \"bucket_train\": false,\n",
    "    \"bucket_val\": false,\n",
    "    \"bucket_test\": false,\n",
    "\n",
    "    \"buckets\": [\n",
    "        [240, 100], [320, 80], [400, 80], [400, 100], [480, 80], [480, 100],\n",
    "        [560, 80], [560, 100], [640, 80], [640, 100], [720, 80], [720, 100],\n",
    "        [720, 120], [720, 200], [800, 100], [800, 320], [1000, 200],\n",
    "        [1000, 400], [1200, 200], [1600, 200], [1600, 1600]\n",
    "        ]\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "# XXX - WARNING - XXX\n",
    "# Processes the full dataset in the background. Takes a few hours.\n",
    "\n",
    "full_data = json.loads(\"\"\"\n",
    "{\n",
    "    \"export_name\": \"data.json\",\n",
    "\n",
    "    \"dir_images_train\": \"../data/processed/formula/images/train/\",\n",
    "    \"dir_images_test\" : \"../data/processed/formula/images/test/\",\n",
    "    \"dir_images_val\"  : \"../data/processed/formula/images/validation/\",\n",
    "\n",
    "    \"path_matching_train\": \"../data/processed/formula/images/train.matching.txt\",\n",
    "    \"path_matching_val\"  : \"../data/processed/formula/images/val.matching.txt\",\n",
    "    \"path_matching_test\" : \"../data/processed/formula/images/test.matching.txt\",\n",
    "\n",
    "    \"path_formulas_train\": \"../data/original/formula/train.formulas.norm.txt\",\n",
    "    \"path_formulas_test\" : \"../data/original/formula/test.formulas.norm.txt\",\n",
    "    \"path_formulas_val\"  : \"../data/original/formula/val.formulas.norm.txt\",\n",
    "\n",
    "    \"bucket_train\": false,\n",
    "    \"bucket_val\": false,\n",
    "    \"bucket_test\": false,\n",
    "\n",
    "    \"max_iter\"          : null,\n",
    "    \"max_length_formula\": 150,\n",
    "\n",
    "    \"buckets\": [\n",
    "        [240, 100], [320, 80], [400, 80], [400, 100], [480, 80], [480, 100],\n",
    "        [560, 80], [560, 100], [640, 80], [640, 100], [720, 80], [720, 100],\n",
    "        [720, 120], [720, 200], [800, 100], [800, 320], [1000, 200],\n",
    "        [1000, 400], [1200, 200], [1600, 200], [1600, 1600]\n",
    "        ]\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "vocab = json.loads(\"\"\"\n",
    "{\n",
    "\t\"export_name\": \"vocab.json\",\n",
    "\n",
    "    \"unk\": \"_UNK\",\n",
    "    \"pad\": \"_PAD\",\n",
    "    \"end\": \"_END\",\n",
    "    \"path_vocab\": \"../data/processed/formula/vocab.txt\",\n",
    "    \"min_count_tok\": 10\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "def build_dataset(data, vocab):\n",
    "    ''' Builds the im2latex dataset\n",
    "    \n",
    "    Arguments:\n",
    "    data - configuration object describing data parameters (see example above)\n",
    "    vocab - configuration object describing the vocabulary parmeters (see example above)\n",
    "    '''\n",
    "    data_config = Config(data)\n",
    "\n",
    "    # datasets\n",
    "    train_set = DataGenerator(\n",
    "        path_formulas=data_config.path_formulas_train,\n",
    "        dir_images=data_config.dir_images_train,\n",
    "        path_matching=data_config.path_matching_train)\n",
    "    test_set  = DataGenerator(\n",
    "        path_formulas=data_config.path_formulas_test,\n",
    "        dir_images=data_config.dir_images_test,\n",
    "        path_matching=data_config.path_matching_test)\n",
    "    val_set   = DataGenerator(\n",
    "        path_formulas=data_config.path_formulas_val,\n",
    "        dir_images=data_config.dir_images_val,\n",
    "        path_matching=data_config.path_matching_val)\n",
    "\n",
    "    # produce images and matching files\n",
    "    train_set.build(buckets=data_config.buckets)\n",
    "    test_set.build(buckets=data_config.buckets)\n",
    "    val_set.build(buckets=data_config.buckets)\n",
    "\n",
    "    # vocab\n",
    "    vocab_config = Config(vocab)\n",
    "    vocab = build_vocab([train_set], min_count=vocab_config.min_count_tok)\n",
    "    write_vocab(vocab, vocab_config.path_vocab)\n",
    "    \n",
    "build_dataset(small_data, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ -  Now that we have generated the dataset, we will put the labels in a form that is easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "### Make sure our data is in order\n",
    "data_base_dir = \"../data\"\n",
    "\n",
    "original_data_path = data_base_dir + \"/original/formula/\"\n",
    "processed_data_path = data_base_dir + \"/processed/formula/\"\n",
    "pickle_data_path = data_base_dir + \"/pickle/formula/\"\n",
    "\n",
    "assert os.path.exists(original_data_path), \"Original data path does not exist.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'../data/processed/formula/images/train/train.matching.txt' does not exist: b'../data/processed/formula/images/train/train.matching.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-79ac48058782>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mvalidation_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtrain_matches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{processed_data_path}images/train/train.matching.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mtest_matches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{processed_data_path}images/test/test.matching.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mvalidation_matches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{processed_data_path}images/validate/val.matching.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml-tf1/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml-tf1/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml-tf1/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml-tf1/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml-tf1/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'../data/processed/formula/images/train/train.matching.txt' does not exist: b'../data/processed/formula/images/train/train.matching.txt'"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "\n",
    "with open(f\"{original_data_path}train.formulas.norm.txt\") as f:\n",
    "    train_labels = np.array(f.readlines())\n",
    "    \n",
    "with open(f\"{original_data_path}test.formulas.norm.txt\") as f:\n",
    "    test_labels = np.array(f.readlines())\n",
    "\n",
    "with open(f\"{original_data_path}val.formulas.norm.txt\") as f:\n",
    "    validation_labels = np.array(f.readlines())\n",
    "\n",
    "train_matches = pd.read_csv(f\"{processed_data_path}images/train/train.matching.txt\", sep=' ', header=None).values\n",
    "test_matches = pd.read_csv(f\"{processed_data_path}images/test/test.matching.txt\", sep=' ', header=None).values    \n",
    "validation_matches = pd.read_csv(f\"{processed_data_path}images/validate/val.matching.txt\", sep=' ', header=None).values\n",
    "\n",
    "print(f\"Found {len(train_labels)} training labels.\")\n",
    "print(f\"Found {len(train_matches)} training matches.\")\n",
    "\n",
    "print(f\"Found {len(test_labels)} test labels.\")\n",
    "print(f\"Found {len(test_matches)} test matches.\")\n",
    "\n",
    "print(f\"Found {len(validation_labels)} validation labels.\")\n",
    "print(f\"Found {len(validation_matches)} validation matches.\")\n",
    "\n",
    "# Get correct labels\n",
    "train_labels = train_labels[[list(map(lambda f: f[1], train_matches))]]\n",
    "test_labels = test_labels[[list(map(lambda f: f[1], test_matches))]]\n",
    "validation_labels = validation_labels[[list(map(lambda f: f[1], validation_matches))]]\n",
    "\n",
    "print(f\"Kept {len(train_labels)} training labels.\")\n",
    "print(f\"Kept {len(test_labels)} test labels.\")\n",
    "print(f\"Kept {len(validation_labels)} validation labels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
