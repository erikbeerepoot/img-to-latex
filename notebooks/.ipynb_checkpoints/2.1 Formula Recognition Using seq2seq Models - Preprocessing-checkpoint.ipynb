{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formula recognition using seq2seq models - Data Preprocessing\n",
    "\n",
    "In this notebook, we describe the steps necessary to post process the data and put it in an easy to consume format.\n",
    "\n",
    "The [im2latex dataset](https://zenodo.org/record/56198#.V2p0KTXT6eA) consists of:\n",
    "> [A] total of ~100k formulas and images splitted into train, validation and test sets. Formulas were parsed from LaTeX sources provided here: http://www.cs.cornell.edu/projects/kddcup/datasets.html(originally from  arXiv). Each image is a PNG image of fixed size. Formula is in black and rest of the image is transparent.\n",
    "\n",
    "The image data provided is not terribly useful due to its large size and transparent background. Luckily, a person undertaking a project with\n",
    "this very same topic produced some [helpful code](https://github.com/guillaumegenthial/im2latex). The code needed some trivial fixes to work with python 3. \n",
    "\n",
    "To generate the dataset, run the code below. It renders each formula into a `.png` file, produces a list of formula to file mappings, an generates the vocabulary.\n",
    "\n",
    "**Note:** Generating the full dataset takes several hours, and will run in the background. You will need to manually kill the python process if you wish to interrupt it. If you do this, get a ‚òïÔ∏è and be prepared to wait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom src import\n",
    "import sys \n",
    "sys.path.append('../src/')\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting click\n",
      "  Using cached https://files.pythonhosted.org/packages/fa/37/45185cb5abbc30d7257104c434fe0b07e5a195a6847506c074527aa599ec/Click-7.0-py2.py3-none-any.whl\n",
      "Installing collected packages: click\n",
      "Successfully installed click-7.0\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install click imageio scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí™ Building dataset... \n",
      "\n",
      "Loaded 10 formulas from ../data/original/formula/small.formulas.norm.txt\n",
      "Loaded 10 formulas from ../data/original/formula/small.formulas.norm.txt\n",
      "Loaded 10 formulas from ../data/original/formula/small.formulas.norm.txt\n",
      "\n",
      "\n",
      "Building vocab...\n",
      "- done. 2/8 tokens added to vocab.\n",
      "Writing vocab...\n",
      "- done. 2 tokens\n",
      "\n",
      " Success! ‚úÖ \n"
     ]
    }
   ],
   "source": [
    "# Adapted from: https://github.com/guillaumegenthial/im2latex\n",
    "\n",
    "import click\n",
    "import json\n",
    "\n",
    "from utils.data_generator import DataGenerator\n",
    "from utils.text import build_vocab, write_vocab\n",
    "from utils.image import build_images\n",
    "from utils.general import Config\n",
    "\n",
    "small_data = json.loads(\"\"\"\n",
    "{\n",
    "    \"export_name\": \"data.json\",\n",
    "\n",
    "    \"dir_images_train\": \"../data/processed/formula/images/train/\",\n",
    "    \"dir_images_test\" : \"../data/processed/formula/images/test/\",\n",
    "    \"dir_images_val\"  : \"../data/processed/formula/images/validate/\",\n",
    "\n",
    "    \"path_matching_train\": \"../data/processed/formula/images/train/train.matching.txt\",\n",
    "    \"path_matching_val\"  : \"../data/processed/formula/images/test/test.matching.txt\",\n",
    "    \"path_matching_test\" : \"../data/processed/formula/images/validate/val.matching.txt\",\n",
    "\n",
    "    \"path_formulas_train\": \"../data/original/formula/small.formulas.norm.txt\",\n",
    "    \"path_formulas_test\" : \"../data/original/formula/small.formulas.norm.txt\",\n",
    "    \"path_formulas_val\"  : \"../data/original/formula/small.formulas.norm.txt\",\n",
    "\n",
    "    \"max_iter\"          : 20,\n",
    "    \"max_length_formula\": 50,\n",
    "\n",
    "    \"bucket_train\": false,\n",
    "    \"bucket_val\": false,\n",
    "    \"bucket_test\": false,\n",
    "\n",
    "    \"buckets\": [\n",
    "        [240, 100], [320, 80], [400, 80], [400, 100], [480, 80], [480, 100],\n",
    "        [560, 80], [560, 100], [640, 80], [640, 100], [720, 80], [720, 100],\n",
    "        [720, 120], [720, 200], [800, 100], [800, 320], [1000, 200],\n",
    "        [1000, 400], [1200, 200], [1600, 200], [1600, 1600]\n",
    "        ]\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "# XXX - WARNING - XXX\n",
    "# Processes the full dataset in the background. Takes a few hours.\n",
    "\n",
    "full_data = json.loads(\"\"\"\n",
    "{\n",
    "    \"export_name\": \"data.json\",\n",
    "\n",
    "    \"dir_images_train\": \"../data/processed/formula/images/train/\",\n",
    "    \"dir_images_test\" : \"../data/processed/formula/images/test/\",\n",
    "    \"dir_images_val\"  : \"../data/processed/formula/images/validation/\",\n",
    "\n",
    "    \"path_matching_train\": \"../data/processed/formula/images/train.matching.txt\",\n",
    "    \"path_matching_val\"  : \"../data/processed/formula/images/val.matching.txt\",\n",
    "    \"path_matching_test\" : \"../data/processed/formula/images/test.matching.txt\",\n",
    "\n",
    "    \"path_formulas_train\": \"../data/original/formula/images/train.formulas.norm.txt\",\n",
    "    \"path_formulas_test\" : \"../data/original/formula/images/test.formulas.norm.txt\",\n",
    "    \"path_formulas_val\"  : \"../data/original/formula/images/val.formulas.norm.txt\",\n",
    "\n",
    "    \"bucket_train\": false,\n",
    "    \"bucket_val\": false,\n",
    "    \"bucket_test\": false,\n",
    "\n",
    "    \"max_iter\"          : null,\n",
    "    \"max_length_formula\": 150,\n",
    "\n",
    "    \"buckets\": [\n",
    "        [240, 100], [320, 80], [400, 80], [400, 100], [480, 80], [480, 100],\n",
    "        [560, 80], [560, 100], [640, 80], [640, 100], [720, 80], [720, 100],\n",
    "        [720, 120], [720, 200], [800, 100], [800, 320], [1000, 200],\n",
    "        [1000, 400], [1200, 200], [1600, 200], [1600, 1600]\n",
    "        ]\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "vocab = json.loads(\"\"\"\n",
    "{\n",
    "\t\"export_name\": \"vocab.json\",\n",
    "\n",
    "    \"unk\": \"_UNK\",\n",
    "    \"pad\": \"_PAD\",\n",
    "    \"end\": \"_END\",\n",
    "    \"path_vocab\": \"../data/processed/formula/vocab.txt\",\n",
    "    \"min_count_tok\": 10\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "def build_dataset(data, vocab):\n",
    "    ''' Builds the im2latex dataset\n",
    "    \n",
    "    Arguments:\n",
    "    data - configuration object describing data parameters (see example above)\n",
    "    vocab - configuration object describing the vocabulary parmeters (see example above)\n",
    "    '''\n",
    "    print(\"üí™ Building dataset... \\n\")\n",
    "    data_config = Config(data)\n",
    "\n",
    "    # datasets\n",
    "    train_set = DataGenerator(\n",
    "        path_formulas=data_config.path_formulas_train,\n",
    "        dir_images=data_config.dir_images_train,\n",
    "        path_matching=data_config.path_matching_train)\n",
    "    test_set  = DataGenerator(\n",
    "        path_formulas=data_config.path_formulas_test,\n",
    "        dir_images=data_config.dir_images_test,\n",
    "        path_matching=data_config.path_matching_test)\n",
    "    val_set   = DataGenerator(\n",
    "        path_formulas=data_config.path_formulas_val,\n",
    "        dir_images=data_config.dir_images_val,\n",
    "        path_matching=data_config.path_matching_val)\n",
    "\n",
    "    # produce images and matching files\n",
    "    train_set.build(buckets=data_config.buckets)\n",
    "    test_set.build(buckets=data_config.buckets)\n",
    "    val_set.build(buckets=data_config.buckets)\n",
    "\n",
    "    # vocab\n",
    "    print(\"\\n\")\n",
    "    vocab_config = Config(vocab)\n",
    "    vocab = build_vocab([train_set], min_count=vocab_config.min_count_tok)\n",
    "    write_vocab(vocab, vocab_config.path_vocab)\n",
    "    \n",
    "build_dataset(small_data, vocab)\n",
    "print(\"\\n Success! ‚úÖ \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have generated the dataset, we will put the labels in a form that is easier to work with. We also filter out any images that do not have a matching groundtruth label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "### Make sure our data is in order\n",
    "data_base_dir = \"../data\"\n",
    "\n",
    "original_data_path = data_base_dir + \"/original/formula/\"\n",
    "processed_data_path = data_base_dir + \"/processed/formula/\"\n",
    "pickle_data_path = data_base_dir + \"/pickle/formula/\"\n",
    "\n",
    "assert os.path.exists(original_data_path), \"Original data path does not exist.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 76322 training labels.\n",
      "Found 10 training matches.\n",
      "Found 9444 test labels.\n",
      "Found 10 test matches.\n",
      "Found 8475 validation labels.\n",
      "Found 10 validation matches.\n",
      "Kept 10 training labels.\n",
      "Kept 10 test labels.\n",
      "Kept 10 validation labels.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikbeerepoot/.virtualenvs/ml-tf1/lib/python3.7/site-packages/ipykernel_launcher.py:28: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "/Users/erikbeerepoot/.virtualenvs/ml-tf1/lib/python3.7/site-packages/ipykernel_launcher.py:29: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "/Users/erikbeerepoot/.virtualenvs/ml-tf1/lib/python3.7/site-packages/ipykernel_launcher.py:30: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "\n",
    "with open(f\"{original_data_path}train.formulas.norm.txt\") as f:\n",
    "    train_labels = np.array(f.readlines())\n",
    "    \n",
    "with open(f\"{original_data_path}test.formulas.norm.txt\") as f:\n",
    "    test_labels = np.array(f.readlines())\n",
    "\n",
    "with open(f\"{original_data_path}val.formulas.norm.txt\") as f:\n",
    "    validation_labels = np.array(f.readlines())\n",
    "\n",
    "train_matches = pd.read_csv(f\"{processed_data_path}images/train/train.matching.txt\", sep=' ', header=None).values\n",
    "test_matches = pd.read_csv(f\"{processed_data_path}images/test/test.matching.txt\", sep=' ', header=None).values    \n",
    "validation_matches = pd.read_csv(f\"{processed_data_path}images/validate/val.matching.txt\", sep=' ', header=None).values\n",
    "\n",
    "print(f\"Found {len(train_labels)} training labels.\")\n",
    "print(f\"Found {len(train_matches)} training matches.\")\n",
    "\n",
    "print(f\"Found {len(test_labels)} test labels.\")\n",
    "print(f\"Found {len(test_matches)} test matches.\")\n",
    "\n",
    "print(f\"Found {len(validation_labels)} validation labels.\")\n",
    "print(f\"Found {len(validation_matches)} validation matches.\")\n",
    "\n",
    "# Get correct labels\n",
    "train_labels = train_labels[[list(map(lambda f: f[1], train_matches))]]\n",
    "test_labels = test_labels[[list(map(lambda f: f[1], test_matches))]]\n",
    "validation_labels = validation_labels[[list(map(lambda f: f[1], validation_matches))]]\n",
    "\n",
    "print(f\"Kept {len(train_labels)} training labels.\")\n",
    "print(f\"Kept {len(test_labels)} test labels.\")\n",
    "print(f\"Kept {len(validation_labels)} validation labels.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
