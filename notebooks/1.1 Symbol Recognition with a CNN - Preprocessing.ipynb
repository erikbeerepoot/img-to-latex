{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Symbol Recognition with a CNN - Preprocessing.ipynb\n",
    "\n",
    "### Dataset description \n",
    "\n",
    "The CROHME 2016 dataset contains ~80k training images, ~70k test images & 12.5k validation images. \n",
    "The images are stored as INKml files, which containthe strokes of the symbol along with the ground truth label. To prepare this dataset for processing, the following steps need to be taken:\n",
    "\n",
    " 1. InkML files were parsed and converted to .png files on disk.\n",
    "\t\t1. Small tweaks were made to the InkML implementation to make the code more readable and fix some small bugs.\n",
    "\t\t2. Spot checks were done after the data was generated to check for obviously corrupt data files.\n",
    " 2. The groundtruth label was extracted from the InkML file and a mapping between the gt label and filename was generated.\n",
    " 3. Each png image was processed by tightly cropping the image, followed by a pad and a resize. Each image in the dataset is now 32x32 pixels. \n",
    "\t\t1. Tensorflow scaling seemed to result in poor quality scaling, with shapes distorted beyond recognizability.l\n",
    "\t\t2. Hence, a cropping & scaling pipeline was developed:\n",
    "\t\t\t1. First, the image was cropped. Since the source data is effectively black and white, this was done by removing all the fully white rows & columns around the outside of the image.\n",
    "\t\t\t2. Second, we applied an erode filter. Since the image is black-on-white, this has the effect of thickening the strokes of the symbols.\n",
    "\t\t\t3. Third, we scaled the image. Empirically, a 64x64 size seemed a good compromise between size and quality. The scaling operation takes into account the aspect ratio, by resizing the longest side of the image first and adjusting the other side to keep the same aspect ratio. This prevents distorting the shapes.\n",
    " 4. At this point, we have our training data (64x64 monochrome png images) and our ground truth labels.\n",
    "\t\t1. The number of unique symbols in this dataset is:\n",
    " 5. Repeat the above steps for test & validation data\n",
    " \n",
    "### Dataset \n",
    " \n",
    "[CROHME 2016 Dataset download link](http://tc11.cvc.uab.es/datasets/ICFHR-CROHME-2016_1)\n",
    " \n",
    "The data can be found (when the archive is extracted) at: `CROHME2016_data/Task-2-Symbols/`. The files in there were extracted and manually split into the `train`, `test` and `validation` sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert InkML files to PNG\n",
    "\n",
    "InkML is a data format that is \"stroke\" based. In other words, the format describes shapes by a series of coordinates that define lines. Plotting each of the lines and saving the figure results in the final input image.\n",
    "\n",
    "With credit to: https://github.com/ThomasLech/CROHME_extractor, whose code was adapted for this project.\n",
    "\n",
    "Below we convert the images from InkML to PNG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom src import\n",
    "import sys \n",
    "sys.path.append('../src/')\n",
    "from inkml import inkml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following project structure is used:\n",
    "```\n",
    "project/\n",
    "    data/ -- Contains the data used in the project (both original and derived)\n",
    "    doc/ -- Project documentation (including report & summary)\n",
    "    figs/ -- Any saved figures generated by the project\n",
    "    notebooks/ -- All notebooks for the project\n",
    "    scripts/ -- Scripts used for various reasons, such as pre-processing\n",
    "    src/ -- Regular python code\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "### Make sure our data is in order\n",
    "data_base_dir = \"../data\"\n",
    "figs_base_dir = \"../figs\"\n",
    "\n",
    "original_data_path = data_base_dir + \"/original/symbol/\"\n",
    "processed_data_path = data_base_dir + \"/processed/symbol/\"\n",
    "pickle_data_path = data_base_dir + \"/pickle/symbol/\"\n",
    "\n",
    "assert os.path.exists(original_data_path), \"Original data path does not exist.\"\n",
    "assert os.path.isdir(processed_data_path), \"Original data path exists, but is not a directory.\"\n",
    "\n",
    "if not os.path.exists(processed_data_path):\n",
    "    print(\"Creating processed data path...\")\n",
    "    os.mkdir(processed_data_path)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_gt_label_key(data_dir):  \n",
    "    ''' \n",
    "    Produce a mapping from the filename to the groundtruth label key\n",
    "    '''\n",
    "    p = Path(data_dir)\n",
    "    paths = list(p.glob(\"**/*.inkml\"))\n",
    "    \n",
    "    print(f\"Parsing groundtruth label keys for {len(paths)} files.\")\n",
    "    \n",
    "    count = 0 \n",
    "    labels = []\n",
    "    for path in paths:\n",
    "        try:\n",
    "            # Remove quotes as they mess up indexing\n",
    "            key = str(inkml.get_groundtruth_label(path.absolute())                        )\n",
    "            key =  key.replace('\\\"','')            \n",
    "            labels.append((os.path.basename(path), key))\n",
    "            count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Encountered exception while processing.\")\n",
    "            print(f\"Src: {path}\")\n",
    "            print(f\"Exception: {e}\")\n",
    "            continue \n",
    "    return labels\n",
    "\n",
    "def parse_gt_labels(data_dir):\n",
    "    '''\n",
    "    Produce a mapping from the label key to the groundtruth label\n",
    "    '''\n",
    "    p = Path(data_dir)\n",
    "    paths = list(p.glob(\"**/*GT.txt\"))\n",
    "    \n",
    "    print(f\"Parsing groundtruth labels for {len(paths)} files.\")\n",
    "    \n",
    "    label_frames = []\n",
    "    for path in paths:\n",
    "        try:\n",
    "            label_frames.append(pd.read_csv(path, header=None))\n",
    "        except Exception as e:\n",
    "            print(f\"Encountered exception while processing.\")\n",
    "            print(f\"Src: {path}\")\n",
    "            print(f\"Exception: {e}\")\n",
    "            continue\n",
    "    complete_frame = pd.concat(label_frames, sort=False)\n",
    "    return complete_frame.apply(lambda str : str.replace('\"',''))\n",
    "\n",
    "def process_data(data_dir, output_dir, generate_count=None, output_type=\".png\"):  \n",
    "    '''\n",
    "    Process the .inkml files and convert to 'output_type' files.\n",
    "    \n",
    "    Arguments:\n",
    "    data_dir -- the directory with the InkML files\n",
    "    output_dir -- the directory for the generated files\n",
    "    generate_count -- maximum number of files to generate\n",
    "    output_type -- the extension of the image type the generate\n",
    "    '''\n",
    "    p = Path(data_dir)\n",
    "    paths = [str(path.absolute()) for path in list(p.glob(\"**/*.inkml\"))]        \n",
    "    target_paths = [path.replace(data_dir, output_dir) for path in paths]\n",
    "    target_paths = [path.replace(\".inkml\", output_type) for path in target_paths]\n",
    "\n",
    "    # Make sure target dirs exist\n",
    "    target_dirs = [os.path.dirname(path) for path in target_paths]\n",
    "    target_dirs = set(target_dirs)\n",
    "    \n",
    "    for target_dir in target_dirs:\n",
    "        if not os.path.exists(target_dir):\n",
    "            print(f\"Creating directory: {target_dir}.\")\n",
    "            Path(target_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if generate_count is None:\n",
    "        print(f\"Converting {len(paths)} inkml files to {output_type}...\")\n",
    "    else:\n",
    "        print(f\"Converting {generate_count} inkml files to {output_type}...\")\n",
    "        \n",
    "    count = 0 \n",
    "    for src, target in zip(paths, target_paths):\n",
    "        try:\n",
    "            if not os.path.exists(target):\n",
    "                inkml.convert_inkml_to_image(src, target)\n",
    "\n",
    "            count += 1        \n",
    "            if count % 500 == 0:\n",
    "                print(f\"{count}/{len(paths)}\")  \n",
    "            if generate_count is not None and count >= generate_count:\n",
    "                return count\n",
    "        except Exception as e:\n",
    "            print(f\"Encountered exception while processing.\")\n",
    "            print(f\"Src: {src}\")\n",
    "            print(f\"Target: {target}\")\n",
    "            print(f\"Exception: {e}\")\n",
    "            continue\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration purposes, below we convert one InkML file into a png, and display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x14976aed0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "src = os.path.abspath(f\"{original_data_path}/train/iso0.inkml\")\n",
    "target = os.path.abspath(f\"{processed_data_path}/train/iso0.png\")\n",
    "inkml.convert_inkml_to_image(src, target)\n",
    "plt.imshow(plt.imread(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 50 inkml files to .png...\n",
      "[train/] Converted 50 files.\n",
      "Converting 50 inkml files to .png...\n",
      "[test/] Converted 50 files.\n",
      "Converting 50 inkml files to .png...\n",
      "[validation/] Converted 50 files.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "###################################\n",
    "### Convert InkML files to pngs ###\n",
    "###################################\n",
    "\n",
    "process_dirs = [\n",
    "    \"train/\",\n",
    "    \"test/\",\n",
    "    \"validation/\"\n",
    "]\n",
    "count = 50\n",
    "\n",
    "for current_dir in process_dirs:    \n",
    "    count = process_data(\n",
    "        f\"{original_data_path}{current_dir}\",\n",
    "        f\"{processed_data_path}{current_dir}\",\n",
    "        generate_count=count\n",
    "    )\n",
    "    print(f\"[{current_dir}] Converted {count} files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing groundtruth labels\n",
    "\n",
    "Below, we parse the ground truth symbol for the images. This is stored alongside the original data in a comma seperated values (csv) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "def parse_groundtruth_labels(groundtruth_path, processed_data_path):\n",
    "    '''\n",
    "    Parse groundtruth labels. This function produces a mapping from:\n",
    "    filename -> path \n",
    "    filename -> groundtruth label\n",
    "    '''\n",
    "    # Get file -> key mapping\n",
    "    filename_to_key_mapping = parse_gt_label_key(groundtruth_path)    \n",
    "    filename_to_path_mapping = { entry[0] : f\"{processed_data_path}{entry[0]}\" for entry in filename_to_key_mapping}\n",
    "\n",
    "    # and key -> label mapping\n",
    "    key_to_label_df = parse_gt_labels(groundtruth_path)\n",
    "    key_to_label_dict = key_to_label_df.set_index(0).T.to_dict('list')\n",
    "    file_to_label_mappings = { file_mapping[0] : key_to_label_dict[file_mapping[1]][0] for file_mapping in filename_to_key_mapping}\n",
    "    \n",
    "    return filename_to_path_mapping, file_to_label_mappings\n",
    "\n",
    "def filter_junk(filename_to_path, filename_to_label):  \n",
    "    '''\n",
    "    Filters out examples with a ground truth label of \"junk\"\n",
    "    '''\n",
    "    filtered_filename_to_path = {}\n",
    "    filtered_filename_to_label = {}\n",
    "    for f_to_p, f_to_l in zip(filename_to_path.items(), filename_to_label.items()):\n",
    "        if f_to_l[1] == \" junk\":\n",
    "            continue\n",
    "        else:\n",
    "            filtered_filename_to_path[f_to_p[0]] = f_to_p[1]\n",
    "            filtered_filename_to_label[f_to_l[0]] = f_to_l[1]\n",
    "    return filtered_filename_to_path, filtered_filename_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing groundtruth label keys for 100 files.\n",
      "Parsing groundtruth labels for 1 files.\n",
      "Found: 100 files with ground truth labels & 13 unique labels. \n",
      "\n",
      "Parsing groundtruth label keys for 100 files.\n",
      "Parsing groundtruth labels for 1 files.\n",
      "Found: 56 files with ground truth labels & 13 unique labels.\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "### Parse ground truth labels for the images ###\n",
    "#################################################\n",
    "\n",
    "training_data_path = f\"{original_data_path}/train/\"\n",
    "processed_train_data_path = f\"{processed_data_path}/train/\"\n",
    "\n",
    "test_data_path = f\"{original_data_path}/test/\"\n",
    "processed_test_data_path = f\"{processed_data_path}/test/\"\n",
    "\n",
    "# Parse ground truth for training data \n",
    "train_filename_to_path, train_filename_to_label = parse_groundtruth_labels(training_data_path, processed_train_data_path)\n",
    "unique_labels = list(set(train_filename_to_label.values()))\n",
    "print(f\"Found: {len(train_filename_to_label)} files with ground truth labels & {len(unique_labels)} unique labels. \\n\")\n",
    "\n",
    "# Parse ground truth for test data \n",
    "test_filename_to_path, test_filename_to_label = parse_groundtruth_labels(test_data_path, processed_test_data_path)\n",
    "test_filename_to_path, test_filename_to_label = filter_junk(test_filename_to_path, test_filename_to_label)\n",
    "test_filename_to_label = dict(test_filename_to_label)\n",
    "print(f\"Found: {len(test_filename_to_label)} files with ground truth labels & {len(unique_labels)} unique labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have:\n",
    "- train_file_to_category -> a mapping between a filename in the training set and the label \n",
    "- test_file_to_category -> a mapping between a filename in the test set and the label \n",
    "- unique_labels -> an array of all the labels in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process images\n",
    "\n",
    "Below, we pad & crop images as appropriate to create a dataset where all images are a uniform size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tensorflow.image import ResizeMethod\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def crop_image(image, min_size=(16,16)):\n",
    "    ''' \n",
    "    Tightly crop image by removing whitespace at edges\n",
    "    '''\n",
    "    # Max values if pixels are all white (croppable space)\n",
    "    max_sum_y = image.shape[0] * 255\n",
    "    max_sum_x = image.shape[1] * 255\n",
    "    \n",
    "    # Compute a mask: 1 if we want to keep, 0 to discard\n",
    "    imsum_x_equal_to_max = np.not_equal(np.sum(image,0), max_sum_y).astype(int)\n",
    "    imsum_y_equal_to_max = np.not_equal(np.sum(image,1), max_sum_x).astype(int)\n",
    "    \n",
    "    # This will compute the index for non-zero entries\n",
    "    # eg. [0,0,1,2,3,4,0,0]\n",
    "    x_indices = np.multiply(imsum_x_equal_to_max.T,np.arange(image.shape[1]))\n",
    "    y_indices = np.multiply(imsum_y_equal_to_max.T,np.arange(image.shape[0]))\n",
    "\n",
    "    # Remove 0 elements\n",
    "    x_indices = x_indices[np.nonzero(x_indices)]\n",
    "    y_indices = y_indices[np.nonzero(y_indices)]\n",
    "    if len(x_indices) == 0 or len(y_indices) == 0:\n",
    "        return image\n",
    "    elif len(x_indices) < len(y_indices) and len(x_indices) < min_size[0] or \\\n",
    "        len(y_indices) < len(x_indices) and len(y_indices) > min_size[1]:\n",
    "        return image\n",
    "    \n",
    "    # Select rows, cols in range\n",
    "    return image[np.min(y_indices):np.max(y_indices),np.min(x_indices):np.max(x_indices),:]\n",
    "    \n",
    "def pad_img(image, size, pad_with=255):\n",
    "    # Make sure we're evenly divisible so we can pad equally on both sides\n",
    "    add_rows = size[0] - image.shape[0]\n",
    "    add_cols = size[1] - image.shape[1]\n",
    "    assert add_rows >= 0, f\"Attempt to crop (add_rows = {add_rows})\"\n",
    "    assert add_cols >= 0, f\"Attempt to crop (add_cols = {add_cols})\"\n",
    "    start_row = int(math.floor(add_rows) / 2)\n",
    "    start_col = int(math.floor(add_cols) / 2)\n",
    "    \n",
    "    # Perform padding\n",
    "    m = np.ones(size) * pad_with\n",
    "    m[\n",
    "        start_row : image.shape[0] + start_row,\n",
    "        start_col : image.shape[1] + start_col\n",
    "    ] = image\n",
    "    return m\n",
    "\n",
    "\n",
    "def resize_img(image, size):\n",
    "    ''' \n",
    "    Check if approximately square\n",
    "    yes ~> resize to 32x32 \n",
    "    no ~> resize longest side to 32, pad the rest\n",
    "    '''\n",
    "    assert size[0]==size[1], \"Dimensions should be equal!\"\n",
    "    \n",
    "    width = image.shape[0] \n",
    "    height = image.shape[1]\n",
    "    aspect_ratio = width / height\n",
    "    \n",
    "    size_before_padding = (0,0)\n",
    "    if math.fabs(1 - aspect_ratio) < 0.1:\n",
    "        # Just scale down evenly\n",
    "        size_before_padding = size\n",
    "    elif aspect_ratio < 1.0:\n",
    "        width = size[0]\n",
    "        height = int(width * aspect_ratio)\n",
    "        size_before_padding = (width, height)\n",
    "    elif aspect_ratio > 1.0:\n",
    "        height = size[1]\n",
    "        width = int(height / aspect_ratio)\n",
    "        size_before_padding = (width, height)\n",
    "\n",
    "    try:\n",
    "        resized_img = cv2.resize(image.numpy(), size_before_padding)\n",
    "        return pad_img(resized_img, size)\n",
    "    except Exception as e:\n",
    "        count = 1\n",
    "\n",
    "    return None\n",
    "\n",
    "def safe_erode(image, kernel=np.ones((5,5), np.uint8)):\n",
    "    try:\n",
    "        im = cv2.erode(cropped_im.numpy(), dilate_kernel)\n",
    "        return im\n",
    "    except:\n",
    "        return image\n",
    "\n",
    "def display_img(image):\n",
    "    color_img = tf.image.grayscale_to_rgb(image)\n",
    "    plt.imshow(color_img)\n",
    "    plt.show()\n",
    "    \n",
    "def is_all_white(image):\n",
    "    np.sum(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 51 images & 51 labels.\n",
      "Got 39 images & 39 labels.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def process_images(file_to_path, file_to_category, erode=True, size=(128,128)):\n",
    "    '''\n",
    "    file_to_path - Dictionary with the imagename as the key and the path as the value\n",
    "    image_to_label - Dictionary with the imagename as the key, and the label as the value\n",
    "    erode - whether or not \"thicken\" the strokes of the images\n",
    "    size - Size of the resulting images \n",
    "    '''\n",
    "    images = np.empty(shape=(len(file_to_category),size[0],size[1]))\n",
    "    labels = []\n",
    "\n",
    "    count = 0\n",
    "    for (image_name, label) in file_to_category.items():\n",
    "        image_path = file_to_path[image_name].replace(\".inkml\", \".png\")\n",
    "        \n",
    "        if not os.path.exists(image_path):\n",
    "            continue\n",
    "\n",
    "        # Read (png) image from disk \n",
    "        img_raw = tf.io.read_file(image_path)\n",
    "        img = tf.image.decode_image(img_raw)\n",
    "\n",
    "        # Even though images are b/w, they're stored in colour. Strip alpha channel, convert\n",
    "        # data type and scale image to [0,1]\n",
    "        img = tf.image.rgb_to_grayscale(img[:,:,0:3])\n",
    "\n",
    "        if is_all_white(img):\n",
    "            continue \n",
    "\n",
    "        # Tightly crop around image (removing some of the whitespace around the edges)\n",
    "        cropped_im = crop_image(img)\n",
    "\n",
    "        # Enhance the writing by thickening the lines using \n",
    "        # the erosion operation (functions like dilate on inverted color)\n",
    "        if erode:\n",
    "            cropped_im = safe_erode(cropped_im)\n",
    "\n",
    "        cropped_and_resized_im = resize_img(cropped_im, size)\n",
    "        if cropped_and_resized_im is not None:\n",
    "            # Construct training array + label\n",
    "            img_tensor = tf.convert_to_tensor(cropped_and_resized_im)\n",
    "            img_tensor = tf.image.convert_image_dtype(img_tensor, dtype=tf.float32)\n",
    "            img_tensor /= 255\n",
    "            images[count] = img_tensor\n",
    "            labels.append(label)\n",
    "            count += 1\n",
    "            \n",
    "    print(f\"Got {count} images & {len(labels)} labels.\")\n",
    "    return np.resize(images, (count,size[0],size[1])), labels\n",
    "\n",
    "train_images, train_labels = process_images(train_filename_to_path, train_filename_to_label)\n",
    "test_images, test_labels = process_images(test_filename_to_path, test_filename_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import re\n",
    "\n",
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "def natural_keys(text):\n",
    "    '''\n",
    "    alist.sort(key=natural_keys) sorts in human order\n",
    "    http://nedbatchelder.com/blog/200712/human_sorting.html\n",
    "    (See Toothy's implementation in the comments)\n",
    "    '''\n",
    "    return [ atoi(c) for c in re.split(r'(\\d+)', text) ]\n",
    "\n",
    "\n",
    "train_file_to_label = sorted(train_filename_to_label.items(), key=lambda x: natural_keys(x[0]))\n",
    "test_filename_to_label = sorted(test_filename_to_label.items(), key=lambda x: natural_keys(x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "To make loading the data easy, we pickle the result. Pickling is pretty feasible here, since the results aren't huge with this data. An alternative would be to use Tensorflow's TFRecord format - more efficient and purpose made.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open(f\"{pickle_data_path}labels.pickle\", 'wb') as handle:\n",
    "    pickle.dump(unique_labels, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(f\"{pickle_data_path}train_labels.pickle\", 'wb') as handle:\n",
    "    pickle.dump(train_labels, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(f\"{pickle_data_path}train_images.pickle\", 'wb') as handle:\n",
    "    pickle.dump(train_images, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(f\"{pickle_data_path}test_labels.pickle\", 'wb') as handle:\n",
    "    pickle.dump(test_labels, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(f\"{pickle_data_path}test_images.pickle\", 'wb') as handle:\n",
    "    pickle.dump(test_images, handle, protocol=pickle.HIGHEST_PROTOCOL)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
